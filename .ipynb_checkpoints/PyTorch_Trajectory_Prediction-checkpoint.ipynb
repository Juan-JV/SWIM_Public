{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  1.13.1\n",
      "Keras version:  2.2.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print ('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import keras\n",
    "print ('Keras version: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import timedelta, date\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14, 'font.weight': 'bold', 'xtick.labelsize': 14, 'axes.labelsize': 14,\n",
    "                    'axes.labelweight': 'bold', 'figure.figsize': (12, 8), 'axes.titlesize': 12})\n",
    "\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "## configure print option\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device in use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device in use:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    for n in range(int((date2 - date1).days) + 1):\n",
    "        yield date1 + timedelta(n)\n",
    "\n",
    "def date_range_list(start_dt, end_dt):\n",
    "    date_list = []\n",
    "    \n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        date = dt.strftime(\"%m%d\")\n",
    "        date_list.append(date)\n",
    "        \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deviation(track_info, target_col, actual_col):\n",
    "    dev = track_info[target_col].astype('float') - track_info[actual_col].astype('float')\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get departure and arrival airport info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport_by_code(ICAO_code):\n",
    "    airports_list = pd.read_csv('airports.dat', header=None, index_col=[0], \n",
    "                      names = ['Name', 'City', 'Country', 'IATA_code', 'ICAO_code', 'Latitude', 'Longitude',\n",
    "                                'Altitude', 'Timezone', 'DST', 'Tz_timezone', 'Type', 'Source'])\n",
    "    \n",
    "    airport = airports_list[airports_list.ICAO_code == ICAO_code]\n",
    "    \n",
    "    if len(airport) == 1:\n",
    "        airport_name = airport['Name'].values[0]\n",
    "        airport_latitude = airport['Latitude'].values[0]\n",
    "        airport_longitude = airport['Longitude'].values[0]\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    return airport_name, airport_latitude, airport_longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build deep learning models: deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare traing and testing data for the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df):\n",
    "    lat_dev = calculate_deviation(df, 'target_Latitude', 'actual_Latitude')\n",
    "    lng_dev = calculate_deviation(df, 'target_Longitude', 'actual_Longitude')\n",
    "    #altitude_dev = calculate_deviation(df, 'targetAltitude', 'actualAltitude')\n",
    "    data = pd.DataFrame({'x_velocity': df['x_velocity'], 'y_velocity': df['y_velocity'], 'speed': df['actualSpeed'], \n",
    "                         'alt': df['actualAltitude'], 'lat_dev': lat_dev, 'lng_dev': lng_dev})\n",
    "    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "def prepare_data(ref_col_name):\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "    \n",
    "    train_index = list(np.arange(len(interpolated_df)))\n",
    "    train_index.remove(test_index)\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    for i in train_index:\n",
    "        new_data = reshape_data(interpolated_df[i])\n",
    "        train_data = pd.concat([train_data, new_data], axis = 0, ignore_index = True)\n",
    "                    \n",
    "    test_data = reshape_data(interpolated_df[test_index])\n",
    "        \n",
    "    #print ('min: ', np.min(train_data[ref_col_name]))\n",
    "    #print ('max: ', np.max(train_data[ref_col_name]))\n",
    "    \n",
    "    X_train = train_data\n",
    "    Y_train = pd.DataFrame(train_data[ref_col_name].shift(-1))\n",
    "\n",
    "    X_train.drop(X_train.index[len(X_train)-1], inplace = True)\n",
    "    Y_train.drop(Y_train.index[len(Y_train)-1], inplace = True)\n",
    "\n",
    "    X_train_n = input_scaler.fit_transform(X_train)\n",
    "    Y_train_n = output_scaler.fit_transform(Y_train)\n",
    "    \n",
    "    X_test = test_data\n",
    "    Y_test = pd.DataFrame(test_data[ref_col_name].shift(-1))\n",
    "    \n",
    "    X_test.drop(X_test.index[len(X_test)-1], inplace=True)\n",
    "    Y_test.drop(Y_test.index[len(Y_test)-1], inplace=True)\n",
    "    \n",
    "    X_test_n = input_scaler.transform(X_test)\n",
    "    Y_test_n = output_scaler.transform(Y_test)\n",
    "    \n",
    "    ind_list = [i for i in range(X_train.shape[0])]\n",
    "    ind_list = shuffle(ind_list)\n",
    "    \n",
    "    X_train_n = X_train_n[ind_list, :]\n",
    "    Y_train_n = Y_train_n[ind_list, :]\n",
    "    \n",
    "    return X_train, Y_train, X_train_n, Y_train_n, X_test_n, Y_test_n, input_scaler, output_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define commonly used function to transform the scaled data back to orignal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(model, file_name, hist):\n",
    "    for key in hist.history.keys():\n",
    "        plt.plot(hist.history[key],label=key)\n",
    "    \n",
    "    plt.title(\"loss={:5.6f}\".format(hist.history[\"loss\"][-1]))\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Iteration\", fontsize = 16)\n",
    "    plt.ylabel(\"Loss\", fontsize = 16)\n",
    "    \n",
    "def transform(scaler, val):\n",
    "    original_val = scaler.inverse_transform(np.reshape(val, newshape = (-1, 1)))\n",
    "    return original_val\n",
    "\n",
    "def convert_to_actual_val(col_name, dev_pred = None):\n",
    "    if dev_pred is None:\n",
    "        res = np.reshape(np.array(interpolated_df[test_index][col_name])[1:], newshape = (-1, 1))\n",
    "        res = res.flatten()\n",
    "    else:\n",
    "        res = np.reshape(np.array(interpolated_df[test_index][col_name])[1:], newshape = (-1, 1)) - dev_pred\n",
    "        res = res.flatten()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_dev_to_lat_lng(tag, dev_val):\n",
    "    if tag == 'latitude':\n",
    "        y_pred_lat_dev = transform(lat_output_scaler, dev_val)\n",
    "        Y_pred = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = y_pred_lat_dev)\n",
    "    elif tag == 'longitude':\n",
    "        y_pred_lng_dev = transform(lng_output_scaler, dev_val)\n",
    "        Y_pred = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = y_pred_lng_dev)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to plot figures for probabilistic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_figures(Y_test, y_pred_do, y_pred_do_mean, scaler, col_name):\n",
    "    plt.rcParams.update({'font.size': 16, 'font.weight': 'bold', 'xtick.labelsize': 14, 'axes.labelsize': 18,\n",
    "                    'axes.labelweight': 'bold', 'figure.figsize': (14, 7), 'axes.titlesize': 12})\n",
    "    f = plt.figure()\n",
    "    for i in range(y_pred_do.shape[0]):\n",
    "        plt.scatter(transform(scaler, Y_test), transform(scaler, y_pred_do[i, :]), c='blue', s = 0.1)\n",
    "\n",
    "    plt.xlabel('Actual ' + col_name +  ' Deviation')\n",
    "    plt.ylabel(col_name + ' Deviation Prediction')\n",
    "    f.savefig(\"DNN_\" + col_name + '_1'+ '.pdf', bbox_inches='tight')\n",
    "    \n",
    "    f = plt.figure()\n",
    "    for i in range(y_pred_do.shape[0]):\n",
    "        sca_true_bayes = plt.scatter(np.arange(0, Y_test.shape[0], 1),\n",
    "                                     transform(scaler, y_pred_do[i, :]), s=0.05, c ='green', alpha = 0.9)\n",
    "\n",
    "    sca_true_val = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, Y_test), s=20, c ='blue', alpha = 0.9)\n",
    "    plt.ylabel(col_name + ' deviation', fontsize = 20, fontweight = 'bold')\n",
    "    plt.legend((sca_true_val, sca_true_bayes), ('True value', 'Prediction'), loc='upper left', ncol=3)\n",
    "    f.savefig(\"DNN_\" + col_name + '_2'+ '.pdf', bbox_inches='tight')\n",
    "    \n",
    "    f = plt.figure()\n",
    "    sca_true = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, Y_test), \n",
    "                           s=12, marker='s', color='b')\n",
    "\n",
    "    sca_pred = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, y_pred_do_mean), \n",
    "                           s=12, marker='^', color='r')\n",
    "\n",
    "    plt.xlabel('Time (Unit: 12 seconds)')\n",
    "    plt.ylabel(col_name + ' deviation', fontsize = 20, fontweight = 'bold')\n",
    "\n",
    "    plt.legend((sca_true, sca_pred), ('True value', 'Prediction Mean'), loc='upper right', ncol=3)\n",
    "    \n",
    "    f.savefig(\"DNN_\" + col_name + '_3'+ '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define multi-step prediction function for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the current deviation prediction to calculate the future latitude and longtitude, \n",
    "## replace the deviation along latitude and longitude in the input vector at the next time instant\n",
    "\n",
    "def predict_multi_steps(steps = 100):\n",
    "    pred_loc = list()\n",
    "    \n",
    "    input_data__ = copy.deepcopy(X_test_lat)\n",
    "    if steps >= Y_test_lat.shape[0]:\n",
    "        print ('Please input a reasonable step size')\n",
    "    else:\n",
    "        for i in range(steps):\n",
    "            #print ('time step ', i, ':', X_test_lat[i, :])  \n",
    "            if i == 0:\n",
    "                data = copy.deepcopy(input_data__[i, :])\n",
    "            else:\n",
    "                data = copy.deepcopy(input_data__[i, :])\n",
    "                data[3] = lat_pred_\n",
    "                data[4] = lng_pred_\n",
    "\n",
    "                        \n",
    "            lat_pred_ = PyTorch_DNN_deterministic_pred(lat_model, np.reshape(data, (1, -1)))\n",
    "            current_lat_pred = transform(lat_output_scaler, lat_pred_)\n",
    "            next_lat = interpolated_df[test_index]['target_Latitude'][i+1] - current_lat_pred\n",
    "                        \n",
    "            lng_pred_ = PyTorch_DNN_deterministic_pred(lng_model, np.reshape(data, (1, -1)))\n",
    "            current_lng_pred = transform(lng_output_scaler, lng_pred_)\n",
    "            next_lng = interpolated_df[test_index]['target_Longitude'][i+1] - current_lng_pred\n",
    "            \n",
    "            pred_loc.append([next_lat[0][0], next_lng[0][0]])\n",
    "        \n",
    "    return pred_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: RNN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_inverse_transform(index, normalized_val, Y_test):\n",
    "    ind = np.arange(index, feature_dim * step_ahead, feature_dim)\n",
    "    \n",
    "    mu = RNN_output_scaler.mean_[ind]\n",
    "    delta = np.sqrt(RNN_output_scaler.var_[ind])\n",
    "    \n",
    "    pred_val = normalized_val * delta + mu\n",
    "    true_val =  Y_test[:, :, index] * delta + mu\n",
    "    \n",
    "    return pred_val, true_val\n",
    "\n",
    "def inverse_RNN_pred(index, normalized_val):\n",
    "    ind = np.arange(index, feature_dim * step_ahead, feature_dim)\n",
    "    \n",
    "    mu = RNN_output_scaler.mean_[ind]\n",
    "    delta = np.sqrt(RNN_output_scaler.var_[ind])\n",
    "    \n",
    "    pred_val = normalized_val * delta + mu\n",
    "    \n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Bayesian RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_probabilistic_prediction(model, X_test, no_output, n_iter=500):\n",
    "    kdp = KerasDropoutPrediction(model, no_output)\n",
    "    y_pred_do = kdp.predict(X_test, n_iter)\n",
    "    y_pred_do_mean = y_pred_do.mean(axis=0)\n",
    "\n",
    "    y_pred = np.array(model.predict(X_test))\n",
    "    \n",
    "    return y_pred_do, y_pred_do_mean, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program\n",
    "\n",
    "The main program has **five major** parts:\n",
    "\n",
    "* Read exported flight trajectory data\n",
    "* Preprocess flight trajectory data: remove duplicated data, interpolate the position data if the duration between two consecutive points is larger than a threshold value\n",
    "* Train two DNN models for flight laitude and longitude deviation prediction, no need to build deviation model for altitude because the altitude between flight plan and actual flight trajectory is almost zero\n",
    "* Train two LSTM RNN models for flight trajectory prediction\n",
    "* Integrate the two trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_trained_model = True\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "feature_dim = 6\n",
    "\n",
    "start_dt = date(2018, 12, 19)\n",
    "end_dt = date(2019, 2, 8)\n",
    "date_list = date_range_list(start_dt, end_dt)\n",
    "\n",
    "date_list.remove('1222')\n",
    "date_list.remove('0118')\n",
    "date_list.remove('0125')\n",
    "date_list.remove('0126')\n",
    "date_list.remove('0129')\n",
    "\n",
    "flightID_dir = ''\n",
    "\n",
    "flight_ID = 'AAL598'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flight_data = 'SFDPS' +'_'+ str(start_dt) + '-' + str(end_dt) + '.pkl'\n",
    "per_flight_df = flight_ID + '_' + str(start_dt) + '-' + str(end_dt) +'.pkl'\n",
    "\n",
    "if os.path.exists(flight_data) and os.path.exists(per_flight_df) and True:\n",
    "    SFDPS_data = open(flight_data, 'rb')\n",
    "    common_flight = pickle.load(SFDPS_data)\n",
    "    \n",
    "    flight_trajec = open(per_flight_df, 'rb')\n",
    "    interpolated_df = pickle.load(flight_trajec)\n",
    "    #show_deviation()\n",
    "else:\n",
    "    common_flight = identify_common_flight()\n",
    "    common_flight = sorted(common_flight, key=str.lower)\n",
    "    \n",
    "    #########################################################################\n",
    "    with open(flight_data, 'wb') as f:\n",
    "         pickle.dump(common_flight, f)\n",
    "    \n",
    "    data = generate_flight_data(flight_ID)\n",
    "    \n",
    "    ########################################################################\n",
    "    interpolated_df = process_trajectory()\n",
    "\n",
    "    ########################################################################\n",
    "    show_deviation()\n",
    "    \n",
    "    with open(per_flight_df, 'wb') as f:\n",
    "        pickle.dump(interpolated_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Build DNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PyTorch_build_DNN_model(X_train, Y_train, dropout = 0.1, learning_rate=0.001, iterations = 10000):\n",
    "    X_train_pt = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "    Y_train_pt = torch.from_numpy(Y_train).type(torch.FloatTensor)\n",
    "    \n",
    "    net_dropped = torch.nn.Sequential(\n",
    "        torch.nn.Linear(feature_dim, 32),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 64),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 32),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 1),\n",
    "    )\n",
    "    \n",
    "    net_dropped.to(device)\n",
    "    \n",
    "    optimizier_drop = torch.optim.Adam(net_dropped.parameters(), lr=learning_rate)\n",
    "    loss_drop = torch.nn.MSELoss()\n",
    "\n",
    "    for t in range(iterations):\n",
    "        out = net_dropped(X_train_pt.to(device))  # input x and predict based on x\n",
    "        loss = loss_drop(out.to(device), Y_train_pt.to(device))  # must be (1. nn output, 2. target)\n",
    "        optimizier_drop.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizier_drop.step()  # apply gradients\n",
    "        \n",
    "        #if t%1000 == 0:\n",
    "        #    print(\"Epoch %d :  %.5f\" % (t, loss))\n",
    "    \n",
    "    return net_dropped\n",
    "\n",
    "def PyTorch_DNN_probabilistic_pred(model, X_test, n_iter):\n",
    "    X_test_pt = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    y_pred_drop = np.zeros(shape = (n_iter, X_test.shape[0]))\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        model.eval()\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith('Dropout'):\n",
    "                model.train()\n",
    "    \n",
    "        y_pred_dropped = model(X_test_pt.to(device))\n",
    "        y_pred_drop[i, :] = torch.Tensor.cpu(y_pred_dropped).data.numpy().reshape(1, X_test.shape[0])\n",
    "    \n",
    "    y_pred_drop_mean = np.mean(y_pred_drop, axis = 0)\n",
    "    \n",
    "    return y_pred_drop, y_pred_drop_mean\n",
    "\n",
    "def PyTorch_DNN_deterministic_pred(model, X_test):\n",
    "    X_test_pt = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = model(X_test_pt.to(device))\n",
    "    y_pred = torch.Tensor.cpu(y_pred).data.numpy().reshape(1, X_test.shape[0])\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN prediction -- combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DNN_combined_results():\n",
    "    no_pred = 5\n",
    "    s_t = 35\n",
    "    plt.rcParams.update({'ytick.labelsize': 14})\n",
    "    \n",
    "    f = plt.figure()\n",
    "    for i in range(n_iter):\n",
    "        lat_dev_pred_ = y_pred_do_lat[i, :]\n",
    "        y_pred_do_lat_p = transform(lat_output_scaler, lat_dev_pred_)\n",
    "        Y_lat_pred_p = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = y_pred_do_lat_p)\n",
    "\n",
    "        lng_dev_pred_ = y_pred_do_lng[i, :]\n",
    "        y_pred_do_lng_p = transform(lng_output_scaler, lng_dev_pred_)\n",
    "        Y_lng_pred_p = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = y_pred_do_lng_p)\n",
    "\n",
    "        prob_ = plt.scatter(Y_lng_pred_p[s_t:no_pred+s_t], Y_lat_pred_p[s_t:no_pred+s_t], s = 60, alpha = 0.1, c = 'magenta', \n",
    "                           label = 'Probabilistic model')\n",
    "\n",
    "        \n",
    "    Lat_plan = convert_to_actual_val(col_name = 'target_Latitude')\n",
    "    Lng_plan = convert_to_actual_val(col_name = 'target_Longitude')\n",
    "    \n",
    "    \n",
    "    prob_mean = plt.scatter(from_dev_to_lat_lng('longitude', y_pred_do_mean_lng)[s_t:no_pred+s_t], \n",
    "                from_dev_to_lat_lng('latitude', y_pred_do_mean_lat)[s_t:no_pred+s_t], s = 130, alpha = 0.8, c = 'black',\n",
    "                marker = 'x', label = 'Mean prediction')\n",
    "\n",
    "    determin_ = plt.scatter(Y_lng_pred[s_t:no_pred+s_t], Y_lat_pred[s_t:no_pred+s_t], s = 50, c = '#85C1E9',\n",
    "                            marker = 'o', label = 'Deterministic model')\n",
    "\n",
    "    actual_ = plt.scatter(Y_lng_true[s_t:no_pred+s_t], Y_lat_true[s_t:no_pred+s_t], s = 50, alpha = 0.9, \n",
    "                          color = 'green', label = 'Actual value')\n",
    "    \n",
    "    plan_ = plt.scatter(Lng_plan[s_t:no_pred+s_t], Lat_plan[s_t:no_pred+s_t], s = 50, alpha = 1, c = '#F39C12', \n",
    "                        marker = 'H', label = 'Filed flight plan')\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    \n",
    "    _ = plt.legend(handles = [prob_, prob_mean, determin_, actual_, plan_], loc = 0)\n",
    "    \n",
    "    \n",
    "    #plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    f.savefig('combined_result.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_deviation_scatter_plot():\n",
    "    #dev_baseline_plt = plt.scatter(lat_dev_baseline, lng_dev_baseline, \n",
    "    #                s = 40, c = 'blue', alpha = 0.8, label = 'Deviation between TT and AT')\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale_view()\n",
    "    \n",
    "    dev_actual_pred_plt = plt.scatter(lat_dev_actual_pred, lng_dev_actual_pred, s = 40, c = 'magenta', \n",
    "                                      alpha = 0.35, label = 'Difference between PT and AT')\n",
    "\n",
    "    plt.xlabel('Latitude deviation')\n",
    "    plt.ylabel('Longitude deviation')\n",
    "    _ = plt.legend(handles = [dev_actual_pred_plt], loc = 2)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_deviation_distribution_plot():\n",
    "    one_lat_dev = calculate_deviation(interpolated_df[test_index], 'target_Latitude', 'actual_Latitude')\n",
    "    one_lng_dev = calculate_deviation(interpolated_df[test_index], 'target_Longitude', 'actual_Longitude')\n",
    "\n",
    "    one_lat_to_miles = 70\n",
    "\n",
    "    f = plt.figure(figsize=(18, 6))\n",
    "    f.add_subplot(121)\n",
    "    plt.hist(one_lat_to_miles * lat_dev_actual_pred.T, alpha = 1, label='DNN Model', color = 'blue', bins = 100)\n",
    "    plt.hist(one_lat_to_miles * DNN_Y_lat_train.values[1:2000], alpha = 0.6, label='Historical', color = 'green', bins = 100)\n",
    "    plt.xlabel('Miles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('The latitude deviation ' + flight_ID, fontsize = 20)\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    one_lng_to_miles = 69\n",
    "    f.add_subplot(122)\n",
    "    plt.hist(one_lat_to_miles * lng_dev_actual_pred.T, alpha=1, label='DNN Model', color = 'blue', bins = 100)\n",
    "    plt.hist(one_lat_to_miles * DNN_Y_lng_train.values[1:2000], alpha=0.6, label='Historical', color = 'green', bins = 100)\n",
    "    plt.xlabel('Miles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('The longitude deviation ' + flight_ID, fontsize = 20)\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty Reduction in DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_uncertainty_reduction():\n",
    "    n_groups = 2\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    sd_baseline = (np.std(lat_dev_baseline), np.std(lng_dev_baseline))\n",
    "    sd_pred_actual = (np.std(lat_dev_actual_pred), np.std(lng_dev_actual_pred))\n",
    "    \n",
    "    plt.figure()\n",
    "    rects1 = plt.bar(index, sd_baseline, bar_width, alpha = opacity, color='b', label='Historical Data')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, sd_pred_actual, bar_width, alpha = opacity, color='g', label='DNN Model')\n",
    "\n",
    "\n",
    "    plt.ylabel(r'$\\sigma$ of trajectory deviation')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_calculate_deviation():\n",
    "\n",
    "    ### baseline \n",
    "    lat_dev_baseline = np.abs(DNN_Y_lat_train.values)\n",
    "    lng_dev_baseline = np.abs(DNN_Y_lng_train.values)\n",
    "\n",
    "    ### model prediction\n",
    "    #test_target_lat = np.array(interpolated_df[test_index]['target_Latitude']).T\n",
    "    #test_target_lng = np.array(interpolated_df[test_index]['target_Longitude']).T\n",
    "\n",
    "    #dev_target_actual_lat = np.abs(test_target_lat[1:n+1] - Y_lat_true[:n].T)\n",
    "    #dev_target_actual_lng = np.abs(test_target_lng[1:n+1] - Y_lng_true[:n].T)\n",
    "    \n",
    "    n = y_pred_do_mean_lat.shape[0]\n",
    "    lat_dev_actual_pred = np.abs(from_dev_to_lat_lng('latitude', y_pred_do_mean_lat)[:n].T - Y_lat_true[:n].T)\n",
    "    lng_dev_actual_pred = np.abs(from_dev_to_lat_lng('longitude', y_pred_do_mean_lng)[:n].T - Y_lng_true[:n].T)\n",
    "\n",
    "    lat_mu_reduction = (np.mean(lat_dev_baseline) - np.mean(lat_dev_actual_pred))/np.mean(lat_dev_baseline)\n",
    "    lng_mu_reduction = (np.mean(lng_dev_baseline) - np.mean(lng_dev_actual_pred))/np.mean(lng_dev_baseline)\n",
    "    \n",
    "    lat_sd_reduction = (np.std(lat_dev_baseline) - np.std(lat_dev_actual_pred))/np.std(lat_dev_baseline)\n",
    "    lng_sd_reduction = (np.std(lng_dev_baseline) - np.std(lng_dev_actual_pred))/np.std(lng_dev_baseline)\n",
    "    \n",
    "    return lat_dev_baseline, lng_dev_baseline, lat_dev_actual_pred, lng_dev_actual_pred, lat_mu_reduction, lng_mu_reduction, \\\n",
    "           lat_sd_reduction, lng_sd_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN multi-step prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_multi_step_prediction():\n",
    "    steps = 10\n",
    "    DNN_multi_pred = predict_multi_steps(steps) ## start from time step 1\n",
    "\n",
    "    DNN_multi_plt = plt.scatter(np.array(DNN_multi_pred)[:, 0], np.array(DNN_multi_pred)[:, 1], \n",
    "                                s = 30, alpha = 0.6, marker = 's', c = 'red', label = 'DNN Prediction')\n",
    "\n",
    "    actual_multi_plt = plt.scatter(Y_lat_true[0:steps], Y_lng_true[0:steps], s = 30, c = 'blue', alpha = 0.6, \n",
    "                                   label = 'Actual Values')\n",
    "\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')\n",
    "    _ = plt.legend(handles = (DNN_multi_plt, actual_multi_plt), loc = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "Local devices [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6091859837515125829\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2969164185\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17605021072504407748\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten, TimeDistributed, Input, GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 2 , 'CPU': 8})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "print (K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Local devices', device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasDropoutPrediction(object):\n",
    "    def __init__(self, model, no_output = 1):\n",
    "        model_output = list()\n",
    "        for i in np.arange(no_output, 0, -1):\n",
    "            model_output.append(model.layers[-i].output)\n",
    "            \n",
    "        self.f = K.function([model.layers[0].input, K.learning_phase()], model_output)\n",
    "        self.model_output = no_output\n",
    "    \n",
    "    def predict(self, x, n_iter=10):\n",
    "        result = []\n",
    "        for _ in range(n_iter):\n",
    "            result.append(self.f([x, 1]))\n",
    "            \n",
    "        if self.model_output > 1:\n",
    "            result = np.array(result).reshape(n_iter, self.model_output, len(x), -1)\n",
    "        else:\n",
    "            result = np.array(result).reshape(n_iter, len(x)).T\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the sliding window size\n",
    "def generate_RNN_data(window_size = 10, step_ahead = 5):\n",
    "    ## Copy the flight trajectory\n",
    "    RNN_input_data = copy.deepcopy(interpolated_df)\n",
    "\n",
    "    ## Drop unnecessary columns\n",
    "    cols_to_drop = ['arrivalPoint', 'airline', 'departurePoint', 'actualPosition', 'targetPosition', \n",
    "                    'target_Latitude', 'target_Longitude', 'targetAltitude', 'actualPositionTime']\n",
    "    \n",
    "    days = len(RNN_input_data)\n",
    "    for i in range(days):\n",
    "        RNN_input_data[i].drop(cols_to_drop, axis = 1, inplace=True)\n",
    "\n",
    "    RNN_train_data = pd.DataFrame()\n",
    "    RNN_test_data = pd.DataFrame()\n",
    "    for i in range(days):\n",
    "        series = pd.DataFrame()\n",
    "        series_s = RNN_input_data[i]\n",
    "        \n",
    "        for j in np.arange(0, window_size + step_ahead):\n",
    "            series = pd.concat([series, series_s.shift(-j)], axis = 1)\n",
    "            \n",
    "        series.dropna(axis = 0, inplace = True)\n",
    "        \n",
    "        if i != test_index:\n",
    "            ## insert the start and end tags for each record\n",
    "            RNN_train_data = pd.concat([RNN_train_data, series])\n",
    "        else:\n",
    "            RNN_test_data = pd.concat([RNN_test_data, series])\n",
    "    \n",
    "    \n",
    "    X_train_data_ = RNN_train_data.iloc[:,:feature_size]\n",
    "    Y_train_data_ = RNN_train_data.iloc[:,feature_size:]\n",
    "    \n",
    "    X_test_data_ = RNN_test_data.iloc[:,:feature_size]\n",
    "    Y_test_data_ = RNN_test_data.iloc[:,feature_size:]\n",
    "        \n",
    "    RNN_input_scaler = StandardScaler()\n",
    "    RNN_output_scaler = StandardScaler()\n",
    "\n",
    "    X_train_data = RNN_input_scaler.fit_transform(X_train_data_)\n",
    "    X_test_data = RNN_input_scaler.transform(X_test_data_)\n",
    "\n",
    "    Y_train_data = RNN_output_scaler.fit_transform(Y_train_data_)\n",
    "    Y_test_data = RNN_output_scaler.transform(Y_test_data_)\n",
    "\n",
    "    X_train = np.array(X_train_data).reshape(X_train_data.shape[0], window_size, feature_dim)\n",
    "    Y_train = np.array(Y_train_data).reshape(Y_train_data.shape[0], step_ahead, feature_dim)\n",
    "\n",
    "    X_test = np.array(X_test_data).reshape(X_test_data.shape[0], window_size, feature_dim)\n",
    "    Y_test = np.array(Y_test_data).reshape(Y_test_data.shape[0], step_ahead, feature_dim)\n",
    "    \n",
    "    return X_train_data_, Y_train_data_, X_test_data_, Y_test_data_, X_train, Y_train, X_test, \\\n",
    "            Y_test, RNN_input_scaler, RNN_output_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_model(idrop, rdrop, odrop, epochs, weight_decay, flag):\n",
    "    inputs = Input(shape=(window_size, feature_dim))\n",
    "\n",
    "    RNN_model = LSTM(50, return_sequences = True, activation = 'relu', kernel_regularizer = l2(weight_decay),\n",
    "                     recurrent_regularizer=l2(weight_decay), dropout=idrop, recurrent_dropout=rdrop)(inputs)\n",
    "    RNN_model = Dropout(odrop)(RNN_model)\n",
    "    \n",
    "    RNN_model = LSTM(50, return_sequences = False, activation = 'relu', kernel_regularizer = l2(weight_decay), \n",
    "                    recurrent_regularizer=l2(weight_decay), dropout=idrop, recurrent_dropout=rdrop)(RNN_model)\n",
    "    RNN_model = Dropout(odrop)(RNN_model)\n",
    "    \n",
    "    RNN_model = keras.layers.Reshape(target_shape = (1, 50))(RNN_model)\n",
    "    \n",
    "    RNN_model = Flatten()(RNN_model)\n",
    "\n",
    "    ## prediction\n",
    "    pred_speed = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_alt = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_x = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_y = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_lat = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_lng = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay))(RNN_model)\n",
    "    \n",
    "    if flag:\n",
    "        RNN_model = Model(inputs=[inputs], outputs=[pred_alt, pred_x, pred_y])\n",
    "\n",
    "        RNN_model.compile(loss=\"mse\", optimizer=optimizers.adam(lr=0.001))\n",
    "        #RNN_model.summary()\n",
    "\n",
    "        RNN_hist = RNN_model.fit(X_train, [Y_train[:, :, 1], Y_train[:, :, 2], Y_train[:, :, 3]], \n",
    "                                 shuffle = True, verbose=0, epochs = epochs, batch_size = 2048)\n",
    "    else:\n",
    "        RNN_model = Model(inputs=[inputs], outputs=[pred_lat, pred_lng])\n",
    "\n",
    "        RNN_model.compile(loss=\"mse\", optimizer=optimizers.adam(lr=0.001))\n",
    "        #RNN_model.summary()\n",
    "\n",
    "        RNN_hist = RNN_model.fit(X_train, [Y_train[:, :, 4], Y_train[:, :, 5]], shuffle = True, \n",
    "                                 verbose=0, epochs = epochs, batch_size = 2048)\n",
    "    \n",
    "    return RNN_model, RNN_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the basic configuration of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "step_ahead = 5\n",
    "RNN_pred_steps = 2\n",
    "\n",
    "feature_size = window_size * feature_dim\n",
    "\n",
    "col_names = ['Speed', 'Altitude', 'x_velocity', 'y_velocity', 'Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of one-step LSTM prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RNN_one_step_position_pred():\n",
    "    plt.figure()\n",
    "    a = np.arange(col_names.index('Latitude'), feature_size, feature_dim)\n",
    "    b = np.arange(col_names.index('Longitude'), feature_size, feature_dim)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 6\n",
    "\n",
    "    legend_input = plt.scatter(X_test_data_.iloc[start_index, a].values, X_test_data_.iloc[start_index, b].values, \n",
    "                               s = 60, c = 'magenta', label = 'Input Data', marker = 'o')\n",
    "\n",
    "    for index_ in range(start_index, end_index):\n",
    "        legend_obs = plt.scatter(lat_test[index_, :], lng_test[index_, :], s = 80, c ='blue', \n",
    "                                 label ='Actual value', marker = 'x')\n",
    "\n",
    "        if index_ == start_index:\n",
    "            legend_pred = plt.scatter(RNN_lat_d[index_, :], RNN_lng_d[index_, :], s = 50, c= 'green', \n",
    "                                      marker = 's', label = 'Prediction')\n",
    "        else:\n",
    "            _ = plt.scatter(RNN_lat_d[index_, 4], RNN_lng_d[index_, 4], s = 50, marker = 's', c= 'green')\n",
    "\n",
    "    _ = plt.legend(handles = [legend_input, legend_obs, legend_pred], loc = 2)\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altitude Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_altitude_prediction():\n",
    "    \n",
    "    a = np.arange(col_names.index('Altitude'), feature_size, feature_dim)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 6\n",
    "\n",
    "    f = plt.figure(figsize = (12, 6))\n",
    "    legend_input = plt.scatter(range(window_size), X_test_data_.iloc[start_index, a].values, s = 60, \n",
    "                               c = 'magenta', label = 'Input Data', marker = 'o')\n",
    "\n",
    "    for index_ in range(start_index, end_index):\n",
    "        if index_ == start_index:\n",
    "            legend_obs = plt.scatter(range(window_size, window_size + step_ahead), \n",
    "                                 alt_test[index_, :], s = 80, c ='blue', label ='Actual Value', marker = 'x')\n",
    "\n",
    "            legend_pred = plt.scatter(range(window_size, window_size + step_ahead), \n",
    "                                      alt_model_pred[index_, :], s = 100, c= 'green', marker = '.', label = 'RNN Prediction')\n",
    "        else:\n",
    "            legend_obs = plt.scatter(range(window_size + step_ahead + index_-1, window_size + step_ahead+ index_), \n",
    "                                 alt_test[index_, 4], s = 80, c ='blue', label ='Actual Value', marker = 'x')\n",
    "\n",
    "            _ = plt.scatter(range(window_size + step_ahead + index_-1, window_size + step_ahead+ index_),\n",
    "                            alt_model_pred[index_, 4], s = 100, marker = '.', c= 'green')\n",
    "\n",
    "    plt.ylabel('Altitude (Unit: feet)', fontsize =16) \n",
    "    plt.xlabel('Time (Unit: 12 seconds)', fontsize =16)\n",
    "    _ = plt.legend(handles = [legend_input, legend_obs, legend_pred], loc = 2)\n",
    "    f.savefig('RNN_Alt.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN_altitude_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_pred_record(input_data, flag, step):\n",
    "    inversed_data = np.zeros((1, step_ahead, feature_dim))\n",
    "    input_data = input_data.reshape(-1, window_size, feature_dim)\n",
    "    \n",
    "    model_state_pred = np.array(state_RNN_model.predict(input_data))\n",
    "    \n",
    "    if step == 0:\n",
    "        model_lat_pred = RNN_lat_mu[row_index + step*step_ahead, :]\n",
    "        model_lng_pred = RNN_lng_mu[row_index + step*step_ahead, :]\n",
    "        model_alt_pred = RNN_alt_mu[row_index + step*step_ahead, :]\n",
    "    else:\n",
    "        Y_model_pred = np.array(lat_lng_RNN_model.predict(input_data))\n",
    "        model_lat_pred = inverse_RNN_pred(col_names.index('Latitude'), Y_model_pred[0, :, :])\n",
    "        model_lng_pred = inverse_RNN_pred(col_names.index('Longitude'), Y_model_pred[1, :, :])\n",
    "        model_alt_pred = inverse_RNN_pred(col_names.index('Altitude'), model_state_pred[0, :, :])\n",
    "\n",
    "    model_x_velocity_pred = inverse_RNN_pred(col_names.index('x_velocity'), model_state_pred[1, :, :])\n",
    "    model_y_velocity_pred = inverse_RNN_pred(col_names.index('y_velocity'), model_state_pred[2, :, :])\n",
    "        \n",
    "    if flag:\n",
    "        model_lat_pred = model_lat_pred + DNN_RNN_correction[0] #+ np.arange(1, 6) * DNN_actual_correction[0]\n",
    "        model_lng_pred = model_lng_pred + DNN_RNN_correction[1] #+ np.arange(1, 6) * DNN_actual_correction[1]\n",
    "        model_alt_pred = model_alt_pred #+ DNN_RNN_correction[2]\n",
    "  \n",
    "    speed = np.sqrt(np.square(model_x_velocity_pred) + np.square(model_y_velocity_pred))\n",
    "    inversed_data[:, :, 0] = speed\n",
    "    inversed_data[:, :, 1] = model_alt_pred\n",
    "    inversed_data[:, :, 2] = model_x_velocity_pred\n",
    "    inversed_data[:, :, 3] = model_y_velocity_pred\n",
    "    inversed_data[:, :, 4] = model_lat_pred\n",
    "    inversed_data[:, :, 5] = model_lng_pred\n",
    "    \n",
    "    return inversed_data.reshape(step_ahead, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_RNN_pred(row_index, flag = True):\n",
    "    RNN_multi_pred = np.zeros((RNN_pred_steps, step_ahead, feature_dim))\n",
    "    \n",
    "    for i in range(RNN_pred_steps):\n",
    "        input_data = copy.deepcopy(X_test_data_.iloc[row_index + i*step_ahead].values)\n",
    "        new_input_data = input_data.reshape(1, window_size * feature_dim)\n",
    "        \n",
    "        if i == 0:\n",
    "            RNN_pred_tm_ = create_RNN_pred_record(RNN_input_scaler.transform(new_input_data), flag, i)\n",
    "        else:\n",
    "            new_input_data[0, (window_size - step_ahead)*feature_dim:window_size*feature_dim] = \\\n",
    "                                        RNN_pred_tm_.reshape(step_ahead * feature_dim, )\n",
    "            \n",
    "            RNN_pred_tm_ = create_RNN_pred_record(RNN_input_scaler.transform(new_input_data), flag, i)\n",
    "        \n",
    "        RNN_multi_pred[i, :, :] = RNN_pred_tm_\n",
    "    \n",
    "    return RNN_multi_pred.reshape(RNN_pred_steps*step_ahead, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrepancy between DNN and RNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discrepancy(row_index):\n",
    "    DNN_RNN_correction = list()\n",
    "    DNN_actual_correction = list()\n",
    "\n",
    "    DNN_index_ = row_index + window_size - 1\n",
    "    \n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~Deviation between DNN and RNN ~~~~~~~~~~~~~')\n",
    "#     print ('Lat true value: ', Y_lat_true[DNN_index_], 'Lng true value: ', Y_lng_true[DNN_index_])\n",
    "#     print ('Lat DNN mean value: ', DNN_lat_mu[DNN_index_], 'Lng DNN mean value: ', DNN_lng_mu[DNN_index_])\n",
    "\n",
    "#     print ('Lat RNN mean value:', RNN_lat_mu[row_index, 0], 'Lng RNN mean value:', RNN_lng_mu[row_index, 0])\n",
    "\n",
    "    lat_dev__ = (DNN_lat_mu[DNN_index_] - RNN_lat_mu[row_index, 0])\n",
    "    lng_dev__ = (DNN_lng_mu[DNN_index_] - RNN_lng_mu[row_index, 0])\n",
    "    alt_dev__ = (DNN_alt_mu[DNN_index_] - RNN_alt_mu[row_index, 0])\n",
    "    DNN_RNN_correction.append([lat_dev__, lng_dev__, alt_dev__])\n",
    "\n",
    "#    print ('Dev value: ', lat_dev__, 'Lng: ', lng_dev__)\n",
    "    \n",
    "    DNN_actual_lat_dev = 0\n",
    "    DNN_actual_lng_dev = 0\n",
    "    \n",
    "    for j in np.arange(1, 6):\n",
    "        DNN_actual_lat_dev += (Y_lat_true[DNN_index_ - j] - DNN_lat_mu[DNN_index_ - j])\n",
    "        DNN_actual_lng_dev += (Y_lng_true[DNN_index_ - j] - DNN_lng_mu[DNN_index_ - j])\n",
    "        \n",
    "    DNN_actual_lat_dev /= 5\n",
    "    DNN_actual_lng_dev /= 5\n",
    "    #print (DNN_actual_lat_dev, DNN_actual_lng_dev)\n",
    "    \n",
    "    DNN_actual_correction.append([DNN_actual_lat_dev, DNN_actual_lng_dev])\n",
    "    \n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~Deviation between actual and DNN ~~~~~~~~~~~~~')\n",
    "#     print ('Actual value:', actual_lat[DNN_index_ - 1], actual_lng[DNN_index_ - 1])\n",
    "#     print ('DNN prediction mean: ', DNN_lat_mu[DNN_index_ - 1], DNN_lng_mu[DNN_index_ - 1])\n",
    "#     print ('Dev value:', actual_lat[DNN_index_ - 1] - DNN_lat_mu[DNN_index_ - 1], \n",
    "#        actual_lng[DNN_index_ - 1] - DNN_lng_mu[DNN_index_ - 1])\n",
    "    \n",
    "#     print ('\\n')\n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~After correction ~~~~~~~~~~~~~')\n",
    "#     print ('After correction: Lat value:', RNN_lat_mu[row_index, 0] + lat_dev__, \n",
    "#           'Lng value:', RNN_lng_mu[row_index, 0] + lng_dev__)\n",
    "#     print ('After correction: Lat value:', RNN_lat_mu[row_index, 0] + lat_dev__ + DNN_actual_lat_dev, \n",
    "#           'Lng value:', RNN_lng_mu[row_index, 0] + lng_dev__ + DNN_actual_lng_dev)\n",
    "\n",
    "    return np.array(DNN_RNN_correction).T, np.array(DNN_actual_correction).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A demo on Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lng_model_comparsion():\n",
    "    \n",
    "    f = plt.figure(figsize = (12, 6))\n",
    "    a = np.arange(4, feature_size, feature_dim)\n",
    "    b = np.arange(5, feature_size, feature_dim)\n",
    "\n",
    "    #input_plt = plt.scatter(X_test_data_.iloc[row_index, a].values, X_test_data_.iloc[row_index, b].values, \n",
    "    #                               s = 80, c = 'magenta', label = 'Input Data', marker = 'X')\n",
    "\n",
    "    In_val = plt.scatter(multi_pred_with_DNN[:, 5], multi_pred_with_DNN[:, 4], s = 80, c= 'blue', label = 'Integrated Model')\n",
    "    RNN_only_ = plt.scatter(multi_pred_no_DNN[:, 5], multi_pred_no_DNN[:, 4], s = 80, c= 'green', label = 'RNN Prediction')\n",
    "    Actual_val = plt.scatter(actual_val[s:s+10, 5], actual_val[s:s+10, 4], marker = '+', c = 'red', s = 90,\n",
    "                             label = 'Actual Value')\n",
    "\n",
    "    _ = plt.legend(handles = [In_val, RNN_only_, Actual_val])\n",
    "    _ = plt.xlabel('Longitude')\n",
    "    _ = plt.ylabel('Latitude')\n",
    "    #plt.ylim([37.125, 37.325])\n",
    "    \n",
    "    \n",
    "    f.savefig('RNN_Lat_Lng_comp.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def altitude_pred_comparison():\n",
    "    a = np.arange(4, feature_size, feature_dim)\n",
    "    b = np.arange(5, feature_size, feature_dim)\n",
    "\n",
    "    f = plt.figure()\n",
    "    In_val = plt.scatter(np.arange(1, 11), multi_pred_with_DNN[:, 1], s = 80, c= 'blue', label = 'Integrated Model')\n",
    "    RNN_only_ = plt.scatter(np.arange(1, 11), multi_pred_no_DNN[:, 1], s = 80, c= 'green', label = 'RNN Only')\n",
    "    Actual_val = plt.scatter(np.arange(1, 11), actual_val[s:s+10, 1], marker = '+', c = 'red', s = 90,\n",
    "                             label = 'Actual Value')\n",
    "\n",
    "    _ = plt.legend(handles = [In_val, RNN_only_, Actual_val])\n",
    "    _ = plt.xlabel('Iterations')\n",
    "    _ = plt.ylabel('Altitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of latitude and longitude deviation of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_deviation_scatter_plot():\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale_view()\n",
    "    \n",
    "    integrated_dev_plt = plt.scatter(integrated_lat_dev, integrated_lng_dev, s = 20, c = 'magenta', alpha = 0.3, \n",
    "                                     label = 'Deviation between IP and AT')\n",
    "\n",
    "    no_DNN_dev = plt.scatter(no_DNN_lat_dev, no_DNN_lng_dev, s = 20, c = 'blue', alpha = 0.15, \n",
    "                             label = 'Deviation between RP and AT')\n",
    "\n",
    "    plt.xlabel('Latitude deviation')\n",
    "    plt.ylabel('Longitude deviation')\n",
    "    _ = plt.legend(handles = [integrated_dev_plt, no_DNN_dev], loc = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caldulate Uncertainty Reduction Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_uncertainty_reduction_visualization():\n",
    "    #############################\n",
    "    #############################\n",
    "    n_groups = 2\n",
    "\n",
    "    # create plot\n",
    "    f = plt.figure(figsize=(14, 7))\n",
    "    f.add_subplot(121)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, mu_no_DNN, bar_width, alpha=opacity, color='b', label='RNN Model')\n",
    "    rects2 = plt.bar(index + bar_width, mu_integrated, bar_width, alpha=opacity, color='g', label='Integrated Model')\n",
    "\n",
    "    plt.ylabel(r'$\\mu$ of trajectory deviation from AT')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    #############################\n",
    "    f.add_subplot(122)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, sd_no_DNN, bar_width, alpha=opacity, color='b', label='RNN Model')\n",
    "    rects2 = plt.bar(index + bar_width, sd_integrated, bar_width, alpha=opacity, color='g', label='Integrated Model')\n",
    "\n",
    "    plt.ylabel(r'$\\sigma$ of trajectory deviation from AT')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute spatial distance from latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    radius = 3958 # unit in miles\n",
    "\n",
    "    dLat = (lat2-lat1) * math.pi / 180\n",
    "    dLng = (lon2-lon1) * math.pi / 180\n",
    "\n",
    "    lat1 = lat1 * math.pi / 180\n",
    "    lat2 = lat2 * math.pi / 180\n",
    "\n",
    "    val = sin(dLat/2) * sin(dLat/2) + sin(dLng/2) * sin(dLng/2) * cos(lat1) * cos(lat2)    \n",
    "    ang = 2 * atan2(sqrt(val), sqrt(1-val))\n",
    "    return radius * ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speed', 'Altitude', 'x_velocity', 'y_velocity', 'Latitude', 'Longitude']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = 1\n",
    "inner_loop = 1\n",
    "\n",
    "DNN_loss = list()\n",
    "DNN_reduction = list()\n",
    "\n",
    "RNN_loss = list()\n",
    "RNN_reduction = list()\n",
    "correction = list()\n",
    "\n",
    "n_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Iter:  0 Inner Iter:  0\n",
      "2019-01-13\n",
      "AAL598-lat_dev_2019-01-13.pth\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "## randomly select a trajectory to test the performance of the algorithm\n",
    "for outer_iter in range(outer_loop):\n",
    "    random_order = shuffle(np.arange(len(interpolated_df)), random_state = outer_iter + 1)\n",
    "    \n",
    "    for inner_iter in range(inner_loop):\n",
    "        print ('Outer Iter: ', outer_iter, 'Inner Iter: ', inner_iter)\n",
    "        \n",
    "        ## Determine test flight trajectory\n",
    "        test_index = random_order[inner_iter]\n",
    "        test_rec_date = str(interpolated_df[test_index]['actualPositionTime'].iloc[0])[:10]\n",
    "        test_rec_date = test_rec_date.strip()\n",
    "        print (test_rec_date)\n",
    "        \n",
    "        #################################################################################\n",
    "        ################################## DNN model for latitude #######################\n",
    "        #################################################################################\n",
    "        lat_col_name = ['lat_dev']\n",
    "        lat_model_name = flight_ID + '-' + ''.join(lat_col_name) + '_' + test_rec_date + '.pth'\n",
    "\n",
    "        DNN_X_lat_train, DNN_Y_lat_train, X_train_lat, Y_train_lat, X_test_lat, Y_test_lat,\\\n",
    "                                    lat_input_scaler, lat_output_scaler = prepare_data(lat_col_name)\n",
    "\n",
    "        print (lat_model_name)\n",
    "        \n",
    "        if os.path.exists(lat_model_name) and use_trained_model:\n",
    "            lat_model = torch.load(lat_model_name)\n",
    "        else:\n",
    "            lat_model = PyTorch_build_DNN_model(X_train_lat, Y_train_lat)\n",
    "            torch.save(lat_model, lat_model_name)\n",
    "\n",
    "        ########### Deterministic DNN Prediction for Latitude ##################    \n",
    "        Y_lat_deter_pred = PyTorch_DNN_deterministic_pred(lat_model, X_test_lat)\n",
    "        Y_lat_dev_pred = transform(lat_output_scaler, Y_lat_deter_pred)\n",
    "        Y_lat_pred = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_dev_pred)\n",
    "        Y_lat_true = convert_to_actual_val(col_name = 'actual_Latitude')\n",
    "\n",
    "        ########### Probabilistic DNN Prediction for Latitude ##################\n",
    "        y_pred_do_lat, y_pred_do_mean_lat = PyTorch_DNN_probabilistic_pred(lat_model, X_test_lat, n_iter)\n",
    "        Y_lat_dev_pred_mean = transform(lat_output_scaler, y_pred_do_mean_lat)\n",
    "        Y_lat_pred_mean_ = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_dev_pred_mean)\n",
    "        \n",
    "        ########################################################################################\n",
    "        ################################## DNN model for longitude #############################\n",
    "        ########################################################################################\n",
    "        lng_col_name = ['lng_dev']\n",
    "        lng_model_name = flight_ID + '-' + ''.join(lng_col_name) + '_' + test_rec_date + '.pth'\n",
    "\n",
    "        DNN_X_lng_train, DNN_Y_lng_train, X_train_lng, Y_train_lng, X_test_lng, Y_test_lng, \\\n",
    "                                    lng_input_scaler, lng_output_scaler = prepare_data(lng_col_name)\n",
    "        \n",
    "        if os.path.exists(lng_model_name) and use_trained_model:\n",
    "            lng_model = torch.load(lng_model_name)\n",
    "        else:\n",
    "            lng_model = PyTorch_build_DNN_model(X_train_lng, Y_train_lng)\n",
    "            torch.save(lng_model, lng_model_name)\n",
    "\n",
    "        ########### Deterministic DNN Prediction for Longitude ################## \n",
    "        Y_lng_deter_pred = PyTorch_DNN_deterministic_pred(lng_model, X_test_lng)\n",
    "        Y_lng_dev_pred = transform(lng_output_scaler, Y_lng_deter_pred)\n",
    "        Y_lng_pred = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_dev_pred)\n",
    "        Y_lng_true = convert_to_actual_val(col_name = 'actual_Longitude')\n",
    "\n",
    "        ### altitude\n",
    "        Y_alt_pred = convert_to_actual_val(col_name = 'targetAltitude')\n",
    "\n",
    "        ########### Probabilistic DNN Prediction for Longitude ##################\n",
    "        y_pred_do_lng, y_pred_do_mean_lng = PyTorch_DNN_probabilistic_pred(lng_model, X_test_lng, n_iter)\n",
    "        Y_lng_dev_pred_mean = transform(lng_output_scaler, y_pred_do_mean_lng)\n",
    "        Y_lng_pred_mean_ = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_dev_pred_mean)\n",
    "        \n",
    "        lat_dev_baseline, lng_dev_baseline, lat_dev_actual_pred, lng_dev_actual_pred, lat_mu_reduction, lng_mu_reduction, \\\n",
    "                        lat_sd_reduction, lng_sd_reduction = DNN_calculate_deviation()\n",
    "        \n",
    "        #DNN_combined_results()\n",
    "        \n",
    "        ###################  DNN Loss   #########################\n",
    "        lat_deter_loss_mse_d = mean_squared_error(Y_lat_true, Y_lat_pred)\n",
    "        lng_deter_loss_mse_d = mean_squared_error(Y_lng_true, Y_lng_pred)\n",
    "        lat_loss_mse_p = mean_squared_error(Y_lat_true, Y_lat_pred_mean_)\n",
    "        lng_loss_mse_p = mean_squared_error(Y_lng_true, Y_lng_pred_mean_)\n",
    "        \n",
    "        lat_deter_loss_mae_d = mean_absolute_error(Y_lat_true, Y_lat_pred)\n",
    "        lng_deter_loss_mae_d = mean_absolute_error(Y_lng_true, Y_lng_pred)\n",
    "        lat_loss_mae_p = mean_absolute_error(Y_lat_true, Y_lat_pred_mean_)\n",
    "        lng_loss_mae_p = mean_absolute_error(Y_lng_true, Y_lng_pred_mean_)\n",
    "        \n",
    "        # calculate spatial distance between prediction and actual values\n",
    "        spatial_deter_dist = []\n",
    "        spatial_prob_dist = []\n",
    "        for i in range(Y_lat_pred.shape[0]):\n",
    "            lat1, lon1, lat2, lon2 = Y_lat_pred[i], Y_lng_pred[i], Y_lat_true[i], Y_lng_true[i]\n",
    "            spatial_deter_dist.append(get_distance(lat1, lon1, lat2, lon2))\n",
    "            \n",
    "            lat1, lon1, lat2, lon2 = Y_lat_pred_mean_[i], Y_lng_pred_mean_[i], Y_lat_true[i], Y_lng_true[i]\n",
    "            spatial_prob_dist.append(get_distance(lat1, lon1, lat2, lon2))\n",
    "            \n",
    "        spatial_deter_loss = np.mean(spatial_deter_dist)\n",
    "        spatial_prob_dist = np.mean(spatial_prob_dist)\n",
    "        \n",
    "        DNN_loss.append([lat_deter_loss_mse_d, lng_deter_loss_mse_d, lat_loss_mse_p, lng_loss_mse_p,\n",
    "                         lat_deter_loss_mae_d, lng_deter_loss_mae_d, lat_loss_mae_p, lng_loss_mae_p,\n",
    "                         spatial_deter_loss, spatial_prob_dist])\n",
    "        \n",
    "        DNN_reduction.append([lat_mu_reduction, lat_sd_reduction, lng_mu_reduction, lng_sd_reduction])\n",
    "        \n",
    "        ##############################################################################\n",
    "        ########################     RNN Lat Lng Model Construction   ################\n",
    "        ##############################################################################\n",
    "        lat_lng_RNN_model_name = flight_ID + '-' + test_rec_date + '-' + 'lat_lng_RNN_model.h5'\n",
    "\n",
    "        X_train_data_, Y_train_data_, X_test_data_, Y_test_data_, X_train, Y_train, X_test, \\\n",
    "            Y_test, RNN_input_scaler, RNN_output_scaler = generate_RNN_data(window_size, step_ahead)\n",
    "\n",
    "        if os.path.exists(lat_lng_RNN_model_name) and use_trained_model:\n",
    "            lat_lng_RNN_model = load_model(lat_lng_RNN_model_name)\n",
    "        else:\n",
    "            ### Latitude & longitude model\n",
    "            lat_lng_RNN_model, RNN_hist = build_RNN_model(idrop = 0, rdrop = 0.1, odrop = 0.1, epochs = 1500,\n",
    "                                                          weight_decay = 1e-5, flag = False)\n",
    "            lat_lng_RNN_model.save(lat_lng_RNN_model_name)\n",
    "        \n",
    "        \n",
    "        ############################## RNN Prediction on Position ##################\n",
    "        no_output = 2\n",
    "\n",
    "        # four dimensions in result: no of samples, no of model output, no of rows in input_data, step_ahead\n",
    "        RNN_pos_p, RNN_pos_p_mu, RNN_pos_d = RNN_probabilistic_prediction(lat_lng_RNN_model, X_test, no_output, n_iter)\n",
    "            \n",
    "        #### Latitude\n",
    "        loc = col_names.index('Latitude')\n",
    "        RNN_lat_d, lat_test = RNN_inverse_transform(loc, RNN_pos_d[0, :, ], Y_test)\n",
    "        #print ('Latitude: ', mean_squared_error(RNN_lat_d, lat_test))\n",
    "\n",
    "        #### Longitude\n",
    "        loc = col_names.index('Longitude')\n",
    "        RNN_lng_d, lng_test = RNN_inverse_transform(loc, RNN_pos_d[1, :, ], Y_test)\n",
    "        #print ('Longitude:', mean_squared_error(RNN_lng_d, lng_test))\n",
    "        \n",
    "        #RNN_one_step_position_pred()\n",
    "        \n",
    "        \n",
    "        #######################################################################\n",
    "        ##################       RNN State Model Construction   ###############\n",
    "        #######################################################################\n",
    "        state_RNN_model_name = flight_ID + '-' + test_rec_date + '-' + 'state_RNN_model.h5'\n",
    "\n",
    "        if os.path.exists(state_RNN_model_name) and use_trained_model:\n",
    "            state_RNN_model = load_model(state_RNN_model_name)\n",
    "        else:\n",
    "            state_RNN_model, RNN_hist = build_RNN_model(idrop = 0, rdrop = 0.1, odrop = 0.1, epochs=1500, \n",
    "                                                        weight_decay = 6e-5, flag = True)\n",
    "            state_RNN_model.save(state_RNN_model_name)\n",
    "            \n",
    "            \n",
    "        ############################################ RNN Prediction on State ####################################\n",
    "        no_output = 3\n",
    "\n",
    "        # four dimensions in result: no of samples, no of model output, no of rows in input_data, step_ahead\n",
    "        RNN_state_p, RNN_state_p_mu, RNN_state_d = RNN_probabilistic_prediction(state_RNN_model, X_test, no_output, n_iter)\n",
    "    \n",
    "        loc = col_names.index('Altitude')\n",
    "        alt_model_pred, alt_test = RNN_inverse_transform(loc, RNN_state_d[0, :, ], Y_test)\n",
    "        #print ('Altitude: ', mean_squared_error(alt_model_pred, alt_test))\n",
    "\n",
    "        loc = col_names.index('x_velocity')\n",
    "        x_model_pred, x_vel_test = RNN_inverse_transform(loc, RNN_state_d[1, :, ], Y_test)\n",
    "        #print ('x_velocity: ', mean_squared_error(x_model_pred, x_vel_test))\n",
    "\n",
    "        loc = col_names.index('y_velocity')\n",
    "        y_model_pred, y_vel_test = RNN_inverse_transform(loc, RNN_state_d[2, :, ], Y_test)\n",
    "        #print ('y_velocity: ', mean_squared_error(y_model_pred, y_vel_test))\n",
    "    \n",
    "        \n",
    "        actual_val = np.array(Y_test_data_)\n",
    "        \n",
    "        ######################################### Uncertainty Reduction in Integrated Model #########################\n",
    "        integrated_lat_dev = list()\n",
    "        integrated_lng_dev = list()\n",
    "        integrated_alt_dev = list()\n",
    "        no_DNN_lat_dev = list()\n",
    "        no_DNN_lng_dev = list()\n",
    "        no_DNN_alt_dev = list()\n",
    "        \n",
    "        #Y_test.shape[0] - step_ahead * RNN_pred_steps\n",
    "        \n",
    "        for s in range(Y_test.shape[0] - 20 * step_ahead * RNN_pred_steps):\n",
    "            #print ('shape', Y_test.shape[0])\n",
    "            #print (s)\n",
    "            row_index = s\n",
    "\n",
    "            DNN_index = row_index + window_size - 1\n",
    "\n",
    "            Y_lat_pred_ = transform(lat_output_scaler, y_pred_do_mean_lat[DNN_index])\n",
    "            Y_lng_pred_ = transform(lng_output_scaler, y_pred_do_mean_lng[DNN_index])\n",
    "\n",
    "            DNN_lat_mu = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_pred_)\n",
    "            DNN_lng_mu = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_pred_)\n",
    "            DNN_alt_mu = Y_alt_pred\n",
    "\n",
    "            ############################### RNN Prediciton  #########################\n",
    "\n",
    "            #### Latitude\n",
    "            loc = col_names.index('Latitude')\n",
    "            RNN_lat_mu = inverse_RNN_pred(loc, RNN_pos_p_mu[0, :, ])\n",
    "\n",
    "            #### Longitude\n",
    "            loc = col_names.index('Longitude')\n",
    "            RNN_lng_mu = inverse_RNN_pred(loc, RNN_pos_p_mu[1, :, ])\n",
    "\n",
    "            #### Altitude\n",
    "            loc = col_names.index('Altitude')\n",
    "            RNN_alt_mu = inverse_RNN_pred(loc, RNN_state_p_mu[0, :, ])\n",
    "\n",
    "            #### X_velocity\n",
    "            loc = col_names.index('x_velocity')\n",
    "            RNN_x_velocity_mu = inverse_RNN_pred(loc, RNN_state_p_mu[1, :, ])\n",
    "\n",
    "            #### Y_velocity\n",
    "            loc = col_names.index('y_velocity')\n",
    "            RNN_y_velocity_mu = inverse_RNN_pred(loc, RNN_state_p_mu[2, :, ])\n",
    "\n",
    "            ########################## Calculate discrepancy  #####################\n",
    "            DNN_RNN_correction, DNN_actual_correction = calculate_discrepancy(row_index)\n",
    "            multi_pred_with_DNN = multi_step_RNN_pred(row_index, flag = True)\n",
    "            multi_pred_no_DNN = multi_step_RNN_pred(row_index, flag = False)\n",
    "            correction.append(DNN_RNN_correction)\n",
    "    \n",
    "            ########################## Record Deviation ##########################\n",
    "            integrated_lat_dev.append(np.abs(multi_pred_with_DNN[:, 4] - actual_val[s:s+10, 4]))\n",
    "            integrated_lng_dev.append(np.abs(multi_pred_with_DNN[:, 5] - actual_val[s:s+10, 5]))\n",
    "            integrated_alt_dev.append(np.abs(multi_pred_with_DNN[:, 1] - actual_val[s:s+10, 1]))\n",
    "            \n",
    "\n",
    "            no_DNN_lat_dev.append(np.abs(multi_pred_no_DNN[:, 4] - actual_val[s:s+10, 4]))\n",
    "            no_DNN_lng_dev.append(np.abs(multi_pred_no_DNN[:, 5] - actual_val[s:s+10, 5]))\n",
    "            no_DNN_alt_dev.append(np.abs(multi_pred_no_DNN[:, 1] - actual_val[s:s+10, 1]))\n",
    "        \n",
    "        integrated_lat_dev = np.concatenate(integrated_lat_dev, axis=0)\n",
    "        integrated_lng_dev = np.concatenate(integrated_lng_dev, axis=0)\n",
    "        integrated_alt_dev = np.concatenate(integrated_alt_dev, axis=0)\n",
    "        \n",
    "        no_DNN_lat_dev = np.concatenate(no_DNN_lat_dev, axis=0)\n",
    "        no_DNN_lng_dev = np.concatenate(no_DNN_lng_dev, axis=0)\n",
    "        no_DNN_alt_dev = np.concatenate(no_DNN_alt_dev, axis=0)\n",
    "        \n",
    "        mu_int_lat_dev = np.mean(integrated_lat_dev)\n",
    "        sd_int_lat_dev = np.std(integrated_lat_dev)\n",
    "\n",
    "        mu_int_lng_dev = np.mean(integrated_lng_dev)\n",
    "        sd_int_lng_dev = np.std(integrated_lng_dev)\n",
    "\n",
    "        mu_no_DNN_lat_dev = np.mean(no_DNN_lat_dev)\n",
    "        sd_no_DNN_lat_dev = np.std(no_DNN_lat_dev)\n",
    "\n",
    "        mu_no_DNN_lng_dev = np.mean(no_DNN_lng_dev)\n",
    "        sd_no_DNN_lng_dev = np.std(no_DNN_lng_dev)\n",
    "\n",
    "        mu_integrated = (mu_int_lat_dev, mu_int_lng_dev)\n",
    "        mu_no_DNN = (mu_no_DNN_lat_dev, mu_no_DNN_lng_dev)\n",
    "\n",
    "        sd_integrated = (sd_int_lat_dev, sd_int_lng_dev)\n",
    "        sd_no_DNN = (sd_no_DNN_lat_dev, sd_no_DNN_lng_dev)\n",
    "\n",
    "        percent_mu_lat = (mu_no_DNN_lat_dev - mu_int_lat_dev)/mu_no_DNN_lat_dev\n",
    "        percent_mu_lng = (mu_no_DNN_lng_dev - mu_int_lng_dev)/mu_no_DNN_lng_dev\n",
    "\n",
    "        percent_sd_lat = (sd_no_DNN_lat_dev - sd_int_lat_dev)/sd_no_DNN_lat_dev\n",
    "        percent_sd_lng = (sd_no_DNN_lng_dev - sd_int_lng_dev)/sd_no_DNN_lng_dev\n",
    "        \n",
    "        \n",
    "        #####################################  RNN Loss #######################################\n",
    "        RNN_lat_loss_mse = np.sqrt(np.mean(np.array(no_DNN_lat_dev)**2))\n",
    "        RNN_lng_loss_mse = np.sqrt(np.mean(np.array(no_DNN_lng_dev)**2))\n",
    "        Integrated_lat_loss_mse = np.sqrt(np.mean(np.array(integrated_lat_dev)**2))\n",
    "        Integrated_lng_loss_mse = np.sqrt(np.mean(np.array(integrated_lng_dev)**2))\n",
    "        RNN_alt_loss_mse = np.sqrt(np.mean(np.array(no_DNN_alt_dev)**2))\n",
    "        x_velocity_loss_mse = mean_squared_error(x_vel_test, x_model_pred)\n",
    "        y_velocity_loss_mse = mean_squared_error(y_vel_test, y_model_pred)\n",
    "        \n",
    "        \n",
    "        RNN_lat_loss_mae = np.mean(no_DNN_lat_dev)\n",
    "        RNN_lng_loss_mae = np.mean(no_DNN_lng_dev)\n",
    "        Integrated_lat_loss_mae = np.mean(integrated_lat_dev)\n",
    "        Integrated_lng_loss_mae = np.mean(integrated_lng_dev)\n",
    "        RNN_alt_loss_mae = np.mean(no_DNN_alt_dev)\n",
    "        x_velocity_loss_mae = mean_absolute_error(x_vel_test, x_model_pred)\n",
    "        y_velocity_loss_mae = mean_absolute_error(y_vel_test, y_model_pred)\n",
    "        \n",
    "        \n",
    "        RNN_loss.append([RNN_lat_loss_mse, RNN_lng_loss_mse, Integrated_lat_loss_mse, Integrated_lng_loss_mse, RNN_alt_loss_mse, \n",
    "                         x_velocity_loss_mse, y_velocity_loss_mse,\n",
    "                         RNN_lat_loss_mae, RNN_lng_loss_mae, Integrated_lat_loss_mae, Integrated_lng_loss_mae, RNN_alt_loss_mae,\n",
    "                         x_velocity_loss_mae, y_velocity_loss_mae])\n",
    "            \n",
    "        RNN_reduction.append([percent_mu_lat, percent_mu_lng, percent_sd_lat, percent_sd_lng])\n",
    "\n",
    "\n",
    "    #RNN_altitude_prediction()\n",
    "    #altitude_pred_comparison()\n",
    "    #lat_lng_model_comparsion()\n",
    "    #RNN_deviation_scatter_plot()\n",
    "    #RNN_uncertainty_reduction_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.298333, -79.174444],\n",
       "       [ 37.284722, -79.191944],\n",
       "       [ 37.268056, -79.215   ],\n",
       "       [ 37.251389, -79.233611],\n",
       "       [ 37.233889, -79.251667],\n",
       "       [ 37.215833, -79.268611],\n",
       "       [ 37.1975  , -79.284167],\n",
       "       [ 37.179167, -79.299722],\n",
       "       [ 37.161111, -79.315   ],\n",
       "       [ 37.142222, -79.330556]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_val[s:s+10, 4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.29714685, -79.17554668],\n",
       "       [ 37.28071252, -79.19302122],\n",
       "       [ 37.26416088, -79.21036148],\n",
       "       [ 37.24747738, -79.22756962],\n",
       "       [ 37.23066738, -79.24463111],\n",
       "       [ 37.21214222, -79.25298164],\n",
       "       [ 37.19586324, -79.27011697],\n",
       "       [ 37.17944115, -79.2870924 ],\n",
       "       [ 37.16286097, -79.30391287],\n",
       "       [ 37.14612746, -79.32056203]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pred_with_DNN[:, 4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_lng_model_comparsion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010886323407642706"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(multi_pred_no_DNN[:, 4:6], actual_val[s:s+10, 4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004013525150348229"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(multi_pred_with_DNN[:, 4:6], actual_val[s:s+10, 4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(flight_ID + '-' + 'results.pkl', 'wb') as f:\n",
    "#      pickle.dump([DNN_loss, DNN_reduction, RNN_loss, RNN_reduction], f)\n",
    "    \n",
    "# with open(flight_ID + '-' + 'correction.pkl', 'wb') as f:\n",
    "#     pickle.dump(correction, f)\n",
    "    \n",
    "# with open(flight_ID + '-' + 'pro_prediction.pkl', 'wb') as f:\n",
    "#     pickle.dump([RNN_pos_p, RNN_state_p], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================  ....DNN model.... =========================\n",
      "====================  Root mean squared error =========================\n",
      "DNN deterministic model: Lat -->  0.0010016401982445946 ; Lng -->  0.001453726720458662\n",
      "DNN probabilistic model: Lat -->  0.0009857683649134317 ; Lng -->  0.0013052240662663045\n",
      "====================  Mean absolute error =========================\n",
      "DNN deterministic model: Lat -->  0.0007330870818129384 ; Lng -->  0.00106853957040155\n",
      "DNN probabilistic model: Lat -->  0.0007178498265715409 ; Lng -->  0.0009530354191834251\n",
      "====================  Spatial distance    =========================\n",
      "DNN deterministic model: spatial dist -->  0.08463201703912901 ; probabilistic model spatial dist -->  0.0782081132729924\n",
      "\n",
      "\n",
      "====================  .... LSTM model .... =========================\n",
      "====================  Root mean squared error =========================\n",
      "RNN LSTM       model: Lat -->  0.03492671527040947 ; Lng -->  0.1686742228882676 ; Alt -->  623.7757383756026 ; X velocity -->  10.961729553374942 ; Y velocity -->  20.428735555207265\n",
      "RNN integrated model: Lat -->  0.014452302157497035 ; Lng -->  0.03875789736473325\n",
      "====================  Mean absolute error =========================\n",
      "RNN LSTM       model: Lat -->  0.03237914351019187 ; Lng -->  0.14623501588201093 ; Alt -->  434.7211287098384 ; X velocity -->  7.35006640278443 ; Y velocity -->  9.77834149766157\n",
      "RNN integrated model: Lat -->  0.00920765831440378 ; Lng -->  0.026415916058869775\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "AA598_res = open(flight_ID + '-results.pkl', 'rb')\n",
    "res = pickle.load(AA598_res)\n",
    "\n",
    "DNN_loss = np.array(res[0])\n",
    "\n",
    "print ('====================  ....DNN model.... =========================')\n",
    "print ('====================  Root mean squared error =========================')\n",
    "print ('DNN deterministic model: Lat --> ', np.mean(np.sqrt(DNN_loss[:, 0])), '; Lng --> ', np.mean(np.sqrt(DNN_loss[:, 1])))\n",
    "print ('DNN probabilistic model: Lat --> ', np.mean(np.sqrt(DNN_loss[:, 2])), '; Lng --> ', np.mean(np.sqrt(DNN_loss[:, 3])))\n",
    "\n",
    "print ('====================  Mean absolute error =========================')\n",
    "print ('DNN deterministic model: Lat --> ', np.mean(DNN_loss[:, 4]), '; Lng --> ', np.mean(DNN_loss[:, 5]))\n",
    "print ('DNN probabilistic model: Lat --> ', np.mean(DNN_loss[:, 6]), '; Lng --> ', np.mean(DNN_loss[:, 7]))\n",
    "\n",
    "print ('====================  Spatial distance    =========================')\n",
    "print ('DNN deterministic model: spatial dist --> ', np.mean(DNN_loss[:, 8]), \n",
    "       '; probabilistic model spatial dist --> ', np.mean(DNN_loss[:, 9]))\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "RNN_loss = np.array(res[2])\n",
    "\n",
    "print ('====================  .... LSTM model .... =========================')\n",
    "print ('====================  Root mean squared error =========================')\n",
    "print ('RNN LSTM       model: Lat --> ', np.mean(RNN_loss[:, 0]), '; Lng --> ', np.mean(RNN_loss[:, 1]), \\\n",
    "                    '; Alt --> ', np.mean(RNN_loss[:, 4]), '; X velocity --> ', np.mean(np.sqrt(RNN_loss[:, 5])), \\\n",
    "                    '; Y velocity --> ', np.mean(np.sqrt(RNN_loss[:, 6])))\n",
    "\n",
    "print ('RNN integrated model: Lat --> ', np.mean(RNN_loss[:, 2]), '; Lng --> ', np.mean(RNN_loss[:, 3]))\n",
    "\n",
    "print ('====================  Mean absolute error =========================')\n",
    "print ('RNN LSTM       model: Lat --> ', np.mean(RNN_loss[:, 7]), '; Lng --> ', np.mean(RNN_loss[:, 8]), \\\n",
    "                    '; Alt --> ', np.mean(RNN_loss[:, 11]), '; X velocity --> ', np.mean(RNN_loss[:, 12]), \\\n",
    "                    '; Y velocity --> ', np.mean(RNN_loss[:, 13]))\n",
    "\n",
    "print ('RNN integrated model: Lat --> ', np.mean(RNN_loss[:, 9]), '; Lng --> ', np.mean(RNN_loss[:, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.834649805033909"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00920765831440378 * 70)**2 + (0.026415916058869775*70)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
