{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version:  2.2.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print ('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import keras\n",
    "print ('Keras version: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14, 'font.weight': 'bold', 'xtick.labelsize': 14, 'axes.labelsize': 14,\n",
    "                    'axes.labelweight': 'bold', 'figure.figsize': (12, 8), 'axes.titlesize': 12})\n",
    "\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "## configure print option\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device in use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device in use:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    for n in range(int((date2 - date1).days) + 1):\n",
    "        yield date1 + timedelta(n)\n",
    "\n",
    "def date_range_list(start_dt, end_dt):\n",
    "    date_list = []\n",
    "    \n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        date = dt.strftime(\"%m%d\")\n",
    "        date_list.append(date)\n",
    "        \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deviation(track_info, target_col, actual_col):\n",
    "    dev = track_info[target_col].astype('float') - track_info[actual_col].astype('float')\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get departure and arrival airport info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport_by_code(ICAO_code):\n",
    "    airports_list = pd.read_csv('airports.dat', header=None, index_col=[0], \n",
    "                      names = ['Name', 'City', 'Country', 'IATA_code', 'ICAO_code', 'Latitude', 'Longitude',\n",
    "                                'Altitude', 'Timezone', 'DST', 'Tz_timezone', 'Type', 'Source'])\n",
    "    \n",
    "    airport = airports_list[airports_list.ICAO_code == ICAO_code]\n",
    "    \n",
    "    if len(airport) == 1:\n",
    "        airport_name = airport['Name'].values[0]\n",
    "        airport_latitude = airport['Latitude'].values[0]\n",
    "        airport_longitude = airport['Longitude'].values[0]\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    return airport_name, airport_latitude, airport_longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build deep learning models: deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare traing and testing data for the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df):\n",
    "    lat_dev = calculate_deviation(df, 'target_Latitude', 'actual_Latitude')\n",
    "    lng_dev = calculate_deviation(df, 'target_Longitude', 'actual_Longitude')\n",
    "    #altitude_dev = calculate_deviation(df, 'targetAltitude', 'actualAltitude')\n",
    "    data = pd.DataFrame({'x_velocity': df['x_velocity'], 'y_velocity': df['y_velocity'], 'speed': df['actualSpeed'], \n",
    "                         'alt': df['actualAltitude'], 'lat_dev': lat_dev, 'lng_dev': lng_dev})\n",
    "    \n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "def prepare_data(ref_col_name):\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "    \n",
    "    train_index = list(np.arange(len(interpolated_df)))\n",
    "    train_index.remove(test_index)\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    for i in train_index:\n",
    "        new_data = reshape_data(interpolated_df[i])\n",
    "        train_data = pd.concat([train_data, new_data], axis = 0, ignore_index = True)\n",
    "                    \n",
    "    test_data = reshape_data(interpolated_df[test_index])\n",
    "        \n",
    "    #print ('min: ', np.min(train_data[ref_col_name]))\n",
    "    #print ('max: ', np.max(train_data[ref_col_name]))\n",
    "    \n",
    "    X_train = train_data\n",
    "    Y_train = pd.DataFrame(train_data[ref_col_name].shift(-1))\n",
    "\n",
    "    X_train.drop(X_train.index[len(X_train)-1], inplace = True)\n",
    "    Y_train.drop(Y_train.index[len(Y_train)-1], inplace = True)\n",
    "\n",
    "    X_train_n = input_scaler.fit_transform(X_train)\n",
    "    Y_train_n = output_scaler.fit_transform(Y_train)\n",
    "    \n",
    "    X_test = test_data\n",
    "    Y_test = pd.DataFrame(test_data[ref_col_name].shift(-1))\n",
    "    \n",
    "    X_test.drop(X_test.index[len(X_test)-1], inplace=True)\n",
    "    Y_test.drop(Y_test.index[len(Y_test)-1], inplace=True)\n",
    "    \n",
    "    X_test_n = input_scaler.transform(X_test)\n",
    "    Y_test_n = output_scaler.transform(Y_test)\n",
    "    \n",
    "    ind_list = [i for i in range(X_train.shape[0])]\n",
    "    ind_list = shuffle(ind_list)\n",
    "    \n",
    "    X_train_n = X_train_n[ind_list, :]\n",
    "    Y_train_n = Y_train_n[ind_list, :]\n",
    "    \n",
    "    return X_train, Y_train, X_train_n, Y_train_n, X_test_n, Y_test_n, input_scaler, output_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define commonly used function to transform the scaled data back to orignal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(model, file_name, hist):\n",
    "    for key in hist.history.keys():\n",
    "        plt.plot(hist.history[key],label=key)\n",
    "    \n",
    "    plt.title(\"loss={:5.6f}\".format(hist.history[\"loss\"][-1]))\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Iteration\", fontsize = 16)\n",
    "    plt.ylabel(\"Loss\", fontsize = 16)\n",
    "    \n",
    "def transform(scaler, val):\n",
    "    original_val = scaler.inverse_transform(np.reshape(val, newshape = (-1, 1)))\n",
    "    return original_val\n",
    "\n",
    "def convert_to_actual_val(col_name, dev_pred = None):\n",
    "    if dev_pred is None:\n",
    "        res = np.reshape(np.array(interpolated_df[test_index][col_name])[1:], newshape = (-1, 1))\n",
    "        res = res.flatten()\n",
    "    else:\n",
    "        res = np.reshape(np.array(interpolated_df[test_index][col_name])[1:], newshape = (-1, 1)) - dev_pred\n",
    "        res = res.flatten()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_dev_to_lat_lng(tag, dev_val):\n",
    "    if tag == 'latitude':\n",
    "        y_pred_lat_dev = transform(lat_output_scaler, dev_val)\n",
    "        Y_pred = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = y_pred_lat_dev)\n",
    "    elif tag == 'longitude':\n",
    "        y_pred_lng_dev = transform(lng_output_scaler, dev_val)\n",
    "        Y_pred = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = y_pred_lng_dev)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to plot figures for probabilistic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_figures(Y_test, y_pred_do, y_pred_do_mean, scaler, col_name):\n",
    "    plt.rcParams.update({'font.size': 16, 'font.weight': 'bold', 'xtick.labelsize': 14, 'axes.labelsize': 18,\n",
    "                    'axes.labelweight': 'bold', 'figure.figsize': (14, 7), 'axes.titlesize': 12})\n",
    "    f = plt.figure()\n",
    "    for i in range(y_pred_do.shape[0]):\n",
    "        plt.scatter(transform(scaler, Y_test), transform(scaler, y_pred_do[i, :]), c='blue', s = 0.1)\n",
    "\n",
    "    plt.xlabel('Actual ' + col_name +  ' Deviation')\n",
    "    plt.ylabel(col_name + ' Deviation Prediction')\n",
    "    f.savefig(\"DNN_\" + col_name + '_1'+ '.pdf', bbox_inches='tight')\n",
    "    \n",
    "    f = plt.figure()\n",
    "    for i in range(y_pred_do.shape[0]):\n",
    "        sca_true_bayes = plt.scatter(np.arange(0, Y_test.shape[0], 1),\n",
    "                                     transform(scaler, y_pred_do[i, :]), s=0.05, c ='green', alpha = 0.9)\n",
    "\n",
    "    sca_true_val = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, Y_test), s=20, c ='blue', alpha = 0.9)\n",
    "    plt.ylabel(col_name + ' deviation', fontsize = 20, fontweight = 'bold')\n",
    "    plt.legend((sca_true_val, sca_true_bayes), ('True value', 'Prediction'), loc='upper left', ncol=3)\n",
    "    f.savefig(\"DNN_\" + col_name + '_2'+ '.pdf', bbox_inches='tight')\n",
    "    \n",
    "    f = plt.figure()\n",
    "    sca_true = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, Y_test), \n",
    "                           s=12, marker='s', color='b')\n",
    "\n",
    "    sca_pred = plt.scatter(np.arange(0, Y_test.shape[0], 1), transform(scaler, y_pred_do_mean), \n",
    "                           s=12, marker='^', color='r')\n",
    "\n",
    "    plt.xlabel('Time (Unit: 12 seconds)')\n",
    "    plt.ylabel(col_name + ' deviation', fontsize = 20, fontweight = 'bold')\n",
    "\n",
    "    plt.legend((sca_true, sca_pred), ('True value', 'Prediction Mean'), loc='upper right', ncol=3)\n",
    "    \n",
    "    f.savefig(\"DNN_\" + col_name + '_3'+ '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define multi-step prediction function for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the current deviation prediction to calculate the future latitude and longtitude, \n",
    "## replace the deviation along latitude and longitude in the input vector at the next time instant\n",
    "\n",
    "def predict_multi_steps(steps = 100):\n",
    "    pred_loc = list()\n",
    "    \n",
    "    input_data__ = copy.deepcopy(X_test_lat)\n",
    "    if steps >= Y_test_lat.shape[0]:\n",
    "        print ('Please input a reasonable step size')\n",
    "    else:\n",
    "        for i in range(steps):\n",
    "            #print ('time step ', i, ':', X_test_lat[i, :])  \n",
    "            if i == 0:\n",
    "                data = copy.deepcopy(input_data__[i, :])\n",
    "            else:\n",
    "                data = copy.deepcopy(input_data__[i, :])\n",
    "                data[3] = lat_pred_\n",
    "                data[4] = lng_pred_\n",
    "\n",
    "                        \n",
    "            lat_pred_ = PyTorch_DNN_deterministic_pred(lat_model, np.reshape(data, (1, -1)))\n",
    "            current_lat_pred = transform(lat_output_scaler, lat_pred_)\n",
    "            next_lat = interpolated_df[test_index]['target_Latitude'][i+1] - current_lat_pred\n",
    "                        \n",
    "            lng_pred_ = PyTorch_DNN_deterministic_pred(lng_model, np.reshape(data, (1, -1)))\n",
    "            current_lng_pred = transform(lng_output_scaler, lng_pred_)\n",
    "            next_lng = interpolated_df[test_index]['target_Longitude'][i+1] - current_lng_pred\n",
    "            \n",
    "            pred_loc.append([next_lat[0][0], next_lng[0][0]])\n",
    "        \n",
    "    return pred_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: RNN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_inverse_transform(index, normalized_val, Y_test):\n",
    "    ind = np.arange(index, feature_dim * step_ahead, feature_dim)\n",
    "    \n",
    "    mu = RNN_output_scaler.mean_[ind]\n",
    "    delta = np.sqrt(RNN_output_scaler.var_[ind])\n",
    "    \n",
    "    pred_val = normalized_val * delta + mu\n",
    "    true_val =  Y_test[:, :, index] * delta + mu\n",
    "    \n",
    "    return pred_val, true_val\n",
    "\n",
    "def inverse_RNN_pred(index, normalized_val):\n",
    "    ind = np.arange(index, feature_dim * step_ahead, feature_dim)\n",
    "    \n",
    "    mu = RNN_output_scaler.mean_[ind]\n",
    "    delta = np.sqrt(RNN_output_scaler.var_[ind])\n",
    "    \n",
    "    pred_val = normalized_val * delta + mu\n",
    "    \n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Bayesian RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_probabilistic_prediction(model, X_test, no_output, n_iter=500):\n",
    "    kdp = KerasDropoutPrediction(model, no_output)\n",
    "    y_pred_do = kdp.predict(X_test, n_iter)\n",
    "    y_pred_do_mean = y_pred_do.mean(axis=0)\n",
    "\n",
    "    y_pred = np.array(model.predict(X_test))\n",
    "    \n",
    "    return y_pred_do, y_pred_do_mean, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program\n",
    "\n",
    "The main program has **five major** parts:\n",
    "\n",
    "* Read exported flight trajectory data\n",
    "* Preprocess flight trajectory data: remove duplicated data, interpolate the position data if the duration between two consecutive points is larger than a threshold value\n",
    "* Train two DNN models for flight laitude and longitude deviation prediction, no need to build deviation model for altitude because the altitude between flight plan and actual flight trajectory is almost zero\n",
    "* Train two LSTM RNN models for flight trajectory prediction\n",
    "* Integrate the two trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_trained_model = True\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "feature_dim = 6\n",
    "\n",
    "start_dt = date(2018, 12, 19)\n",
    "end_dt = date(2019, 2, 8)\n",
    "date_list = date_range_list(start_dt, end_dt)\n",
    "\n",
    "date_list.remove('1222')\n",
    "date_list.remove('0118')\n",
    "date_list.remove('0125')\n",
    "date_list.remove('0126')\n",
    "date_list.remove('0129')\n",
    "\n",
    "flightID_dir = ''\n",
    "\n",
    "flight_ID = 'AAL598'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flight_data = 'SFDPS' +'_'+ str(start_dt) + '-' + str(end_dt) + '.pkl'\n",
    "per_flight_df = flight_ID + '_' + str(start_dt) + '-' + str(end_dt) +'.pkl'\n",
    "\n",
    "if os.path.exists(flight_data) and os.path.exists(per_flight_df) and True:\n",
    "    SFDPS_data = open(flight_data, 'rb')\n",
    "    common_flight = pickle.load(SFDPS_data)\n",
    "    \n",
    "    flight_trajec = open(per_flight_df, 'rb')\n",
    "    interpolated_df = pickle.load(flight_trajec)\n",
    "    #show_deviation()\n",
    "else:\n",
    "    common_flight = identify_common_flight()\n",
    "    common_flight = sorted(common_flight, key=str.lower)\n",
    "    \n",
    "    #########################################################################\n",
    "    with open(flight_data, 'wb') as f:\n",
    "         pickle.dump(common_flight, f)\n",
    "    \n",
    "    data = generate_flight_data(flight_ID)\n",
    "    \n",
    "    ########################################################################\n",
    "    interpolated_df = process_trajectory()\n",
    "\n",
    "    ########################################################################\n",
    "    show_deviation()\n",
    "    \n",
    "    with open(per_flight_df, 'wb') as f:\n",
    "        pickle.dump(interpolated_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Build DNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PyTorch_build_DNN_model(X_train, Y_train, dropout = 0.1, learning_rate=0.001, iterations = 10000):\n",
    "    X_train_pt = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "    Y_train_pt = torch.from_numpy(Y_train).type(torch.FloatTensor)\n",
    "    \n",
    "    net_dropped = torch.nn.Sequential(\n",
    "        torch.nn.Linear(feature_dim, 32),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 64),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 32),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 1),\n",
    "    )\n",
    "    \n",
    "    net_dropped.to(device)\n",
    "    \n",
    "    optimizier_drop = torch.optim.Adam(net_dropped.parameters(), lr=learning_rate)\n",
    "    loss_drop = torch.nn.MSELoss()\n",
    "\n",
    "    for t in range(iterations):\n",
    "        out = net_dropped(X_train_pt.to(device))  # input x and predict based on x\n",
    "        loss = loss_drop(out.to(device), Y_train_pt.to(device))  # must be (1. nn output, 2. target)\n",
    "        optimizier_drop.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizier_drop.step()  # apply gradients\n",
    "        \n",
    "        #if t%1000 == 0:\n",
    "        #    print(\"Epoch %d :  %.5f\" % (t, loss))\n",
    "    \n",
    "    return net_dropped\n",
    "\n",
    "def PyTorch_DNN_probabilistic_pred(model, X_test, n_iter):\n",
    "    X_test_pt = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    y_pred_drop = np.zeros(shape = (n_iter, X_test.shape[0]))\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        model.eval()\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith('Dropout'):\n",
    "                model.train()\n",
    "    \n",
    "        y_pred_dropped = model(X_test_pt.to(device))\n",
    "        y_pred_drop[i, :] = torch.Tensor.cpu(y_pred_dropped).data.numpy().reshape(1, X_test.shape[0])\n",
    "    \n",
    "    y_pred_drop_mean = np.mean(y_pred_drop, axis = 0)\n",
    "    \n",
    "    return y_pred_drop, y_pred_drop_mean\n",
    "\n",
    "def PyTorch_DNN_deterministic_pred(model, X_test):\n",
    "    X_test_pt = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = model(X_test_pt.to(device))\n",
    "    y_pred = torch.Tensor.cpu(y_pred).data.numpy().reshape(1, X_test.shape[0])\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN prediction -- combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DNN_combined_results():\n",
    "    no_pred = 5\n",
    "    s_t = 35\n",
    "    plt.rcParams.update({'ytick.labelsize': 14})\n",
    "    \n",
    "    f = plt.figure()\n",
    "    for i in range(n_iter):\n",
    "        lat_dev_pred_ = y_pred_do_lat[i, :]\n",
    "        y_pred_do_lat_p = transform(lat_output_scaler, lat_dev_pred_)\n",
    "        Y_lat_pred_p = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = y_pred_do_lat_p)\n",
    "\n",
    "        lng_dev_pred_ = y_pred_do_lng[i, :]\n",
    "        y_pred_do_lng_p = transform(lng_output_scaler, lng_dev_pred_)\n",
    "        Y_lng_pred_p = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = y_pred_do_lng_p)\n",
    "\n",
    "        prob_ = plt.scatter(Y_lng_pred_p[s_t:no_pred+s_t], Y_lat_pred_p[s_t:no_pred+s_t], s = 60, alpha = 0.1, c = 'magenta', \n",
    "                           label = 'Probabilistic model')\n",
    "\n",
    "        \n",
    "    Lat_plan = convert_to_actual_val(col_name = 'target_Latitude')\n",
    "    Lng_plan = convert_to_actual_val(col_name = 'target_Longitude')\n",
    "    \n",
    "    \n",
    "    prob_mean = plt.scatter(from_dev_to_lat_lng('longitude', y_pred_do_mean_lng)[s_t:no_pred+s_t], \n",
    "                from_dev_to_lat_lng('latitude', y_pred_do_mean_lat)[s_t:no_pred+s_t], s = 130, alpha = 0.8, c = 'black',\n",
    "                marker = 'x', label = 'Mean prediction')\n",
    "\n",
    "    determin_ = plt.scatter(Y_lng_pred[s_t:no_pred+s_t], Y_lat_pred[s_t:no_pred+s_t], s = 50, c = '#85C1E9',\n",
    "                            marker = 'o', label = 'Deterministic model')\n",
    "\n",
    "    actual_ = plt.scatter(Y_lng_true[s_t:no_pred+s_t], Y_lat_true[s_t:no_pred+s_t], s = 50, alpha = 0.9, \n",
    "                          color = 'green', label = 'Actual value')\n",
    "    \n",
    "    plan_ = plt.scatter(Lng_plan[s_t:no_pred+s_t], Lat_plan[s_t:no_pred+s_t], s = 50, alpha = 1, c = '#F39C12', \n",
    "                        marker = 'H', label = 'Filed flight plan')\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    \n",
    "    _ = plt.legend(handles = [prob_, prob_mean, determin_, actual_, plan_], loc = 0)\n",
    "    \n",
    "    \n",
    "    #plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    f.savefig('combined_result.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_deviation_scatter_plot():\n",
    "    #dev_baseline_plt = plt.scatter(lat_dev_baseline, lng_dev_baseline, \n",
    "    #                s = 40, c = 'blue', alpha = 0.8, label = 'Deviation between TT and AT')\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale_view()\n",
    "    \n",
    "    dev_actual_pred_plt = plt.scatter(lat_dev_actual_pred, lng_dev_actual_pred, s = 40, c = 'magenta', \n",
    "                                      alpha = 0.35, label = 'Difference between PT and AT')\n",
    "\n",
    "    plt.xlabel('Latitude deviation')\n",
    "    plt.ylabel('Longitude deviation')\n",
    "    _ = plt.legend(handles = [dev_actual_pred_plt], loc = 2)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_deviation_distribution_plot():\n",
    "    one_lat_dev = calculate_deviation(interpolated_df[test_index], 'target_Latitude', 'actual_Latitude')\n",
    "    one_lng_dev = calculate_deviation(interpolated_df[test_index], 'target_Longitude', 'actual_Longitude')\n",
    "\n",
    "    one_lat_to_miles = 70\n",
    "\n",
    "    f = plt.figure(figsize=(18, 6))\n",
    "    f.add_subplot(121)\n",
    "    plt.hist(one_lat_to_miles * lat_dev_actual_pred.T, alpha = 1, label='DNN Model', color = 'blue', bins = 100)\n",
    "    plt.hist(one_lat_to_miles * DNN_Y_lat_train.values[1:2000], alpha = 0.6, label='Historical', color = 'green', bins = 100)\n",
    "    plt.xlabel('Miles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('The latitude deviation ' + flight_ID, fontsize = 20)\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    one_lng_to_miles = 69\n",
    "    f.add_subplot(122)\n",
    "    plt.hist(one_lat_to_miles * lng_dev_actual_pred.T, alpha=1, label='DNN Model', color = 'blue', bins = 100)\n",
    "    plt.hist(one_lat_to_miles * DNN_Y_lng_train.values[1:2000], alpha=0.6, label='Historical', color = 'green', bins = 100)\n",
    "    plt.xlabel('Miles')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('The longitude deviation ' + flight_ID, fontsize = 20)\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty Reduction in DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_uncertainty_reduction():\n",
    "    n_groups = 2\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    sd_baseline = (np.std(lat_dev_baseline), np.std(lng_dev_baseline))\n",
    "    sd_pred_actual = (np.std(lat_dev_actual_pred), np.std(lng_dev_actual_pred))\n",
    "    \n",
    "    plt.figure()\n",
    "    rects1 = plt.bar(index, sd_baseline, bar_width, alpha = opacity, color='b', label='Historical Data')\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, sd_pred_actual, bar_width, alpha = opacity, color='g', label='DNN Model')\n",
    "\n",
    "\n",
    "    plt.ylabel(r'$\\sigma$ of trajectory deviation')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_calculate_deviation():\n",
    "\n",
    "    ### baseline \n",
    "    lat_dev_baseline = np.abs(DNN_Y_lat_train.values)\n",
    "    lng_dev_baseline = np.abs(DNN_Y_lng_train.values)\n",
    "\n",
    "    ### model prediction\n",
    "    #test_target_lat = np.array(interpolated_df[test_index]['target_Latitude']).T\n",
    "    #test_target_lng = np.array(interpolated_df[test_index]['target_Longitude']).T\n",
    "\n",
    "    #dev_target_actual_lat = np.abs(test_target_lat[1:n+1] - Y_lat_true[:n].T)\n",
    "    #dev_target_actual_lng = np.abs(test_target_lng[1:n+1] - Y_lng_true[:n].T)\n",
    "    \n",
    "    n = y_pred_do_mean_lat.shape[0]\n",
    "    lat_dev_actual_pred = np.abs(from_dev_to_lat_lng('latitude', y_pred_do_mean_lat)[:n].T - Y_lat_true[:n].T)\n",
    "    lng_dev_actual_pred = np.abs(from_dev_to_lat_lng('longitude', y_pred_do_mean_lng)[:n].T - Y_lng_true[:n].T)\n",
    "\n",
    "    lat_mu_reduction = (np.mean(lat_dev_baseline) - np.mean(lat_dev_actual_pred))/np.mean(lat_dev_baseline)\n",
    "    lng_mu_reduction = (np.mean(lng_dev_baseline) - np.mean(lng_dev_actual_pred))/np.mean(lng_dev_baseline)\n",
    "    \n",
    "    lat_sd_reduction = (np.std(lat_dev_baseline) - np.std(lat_dev_actual_pred))/np.std(lat_dev_baseline)\n",
    "    lng_sd_reduction = (np.std(lng_dev_baseline) - np.std(lng_dev_actual_pred))/np.std(lng_dev_baseline)\n",
    "    \n",
    "    return lat_dev_baseline, lng_dev_baseline, lat_dev_actual_pred, lng_dev_actual_pred, lat_mu_reduction, lng_mu_reduction, \\\n",
    "           lat_sd_reduction, lng_sd_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN multi-step prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_multi_step_prediction():\n",
    "    steps = 10\n",
    "    DNN_multi_pred = predict_multi_steps(steps) ## start from time step 1\n",
    "\n",
    "    DNN_multi_plt = plt.scatter(np.array(DNN_multi_pred)[:, 0], np.array(DNN_multi_pred)[:, 1], \n",
    "                                s = 30, alpha = 0.6, marker = 's', c = 'red', label = 'DNN Prediction')\n",
    "\n",
    "    actual_multi_plt = plt.scatter(Y_lat_true[0:steps], Y_lng_true[0:steps], s = 30, c = 'blue', alpha = 0.6, \n",
    "                                   label = 'Actual Values')\n",
    "\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')\n",
    "    _ = plt.legend(handles = (DNN_multi_plt, actual_multi_plt), loc = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "Local devices [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18220417899310990849\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2958934016\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3244230486831153066\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten, TimeDistributed, Input, GRU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 2 , 'CPU': 8})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "print (K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print('Local devices', device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasDropoutPrediction(object):\n",
    "    def __init__(self, model, no_output = 1):\n",
    "        model_output = list()\n",
    "        for i in np.arange(no_output, 0, -1):\n",
    "            model_output.append(model.layers[-i].output)\n",
    "            \n",
    "        self.f = K.function([model.layers[0].input, K.learning_phase()], model_output)\n",
    "        self.model_output = no_output\n",
    "    \n",
    "    def predict(self, x, n_iter=10):\n",
    "        result = []\n",
    "        for _ in range(n_iter):\n",
    "            result.append(self.f([x, 1]))\n",
    "            \n",
    "        if self.model_output > 1:\n",
    "            result = np.array(result).reshape(n_iter, self.model_output, len(x), -1)\n",
    "        else:\n",
    "            result = np.array(result).reshape(n_iter, len(x)).T\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the sliding window size\n",
    "def generate_RNN_data(window_size = 10, step_ahead = 5):\n",
    "    ## Copy the flight trajectory\n",
    "    RNN_input_data = copy.deepcopy(interpolated_df)\n",
    "\n",
    "    ## Drop unnecessary columns\n",
    "    cols_to_drop = ['arrivalPoint', 'airline', 'departurePoint', 'actualPosition', 'targetPosition', \n",
    "                    'target_Latitude', 'target_Longitude', 'targetAltitude', 'actualPositionTime']\n",
    "    \n",
    "    days = len(RNN_input_data)\n",
    "    for i in range(days):\n",
    "        RNN_input_data[i].drop(cols_to_drop, axis = 1, inplace=True)\n",
    "\n",
    "    RNN_train_data = pd.DataFrame()\n",
    "    RNN_test_data = pd.DataFrame()\n",
    "    for i in range(days):\n",
    "        series = pd.DataFrame()\n",
    "        series_s = RNN_input_data[i]\n",
    "        \n",
    "        for j in np.arange(0, window_size + step_ahead):\n",
    "            series = pd.concat([series, series_s.shift(-j)], axis = 1)\n",
    "            \n",
    "        series.dropna(axis = 0, inplace = True)\n",
    "        \n",
    "        if i != test_index:\n",
    "            ## insert the start and end tags for each record\n",
    "            RNN_train_data = pd.concat([RNN_train_data, series])\n",
    "        else:\n",
    "            RNN_test_data = pd.concat([RNN_test_data, series])\n",
    "    \n",
    "    \n",
    "    X_train_data_ = RNN_train_data.iloc[:,:feature_size]\n",
    "    Y_train_data_ = RNN_train_data.iloc[:,feature_size:]\n",
    "    \n",
    "    X_test_data_ = RNN_test_data.iloc[:,:feature_size]\n",
    "    Y_test_data_ = RNN_test_data.iloc[:,feature_size:]\n",
    "        \n",
    "    RNN_input_scaler = StandardScaler()\n",
    "    RNN_output_scaler = StandardScaler()\n",
    "\n",
    "    X_train_data = RNN_input_scaler.fit_transform(X_train_data_)\n",
    "    X_test_data = RNN_input_scaler.transform(X_test_data_)\n",
    "\n",
    "    Y_train_data = RNN_output_scaler.fit_transform(Y_train_data_)\n",
    "    Y_test_data = RNN_output_scaler.transform(Y_test_data_)\n",
    "\n",
    "    X_train = np.array(X_train_data).reshape(X_train_data.shape[0], window_size, feature_dim)\n",
    "    Y_train = np.array(Y_train_data).reshape(Y_train_data.shape[0], step_ahead, feature_dim)\n",
    "\n",
    "    X_test = np.array(X_test_data).reshape(X_test_data.shape[0], window_size, feature_dim)\n",
    "    Y_test = np.array(Y_test_data).reshape(Y_test_data.shape[0], step_ahead, feature_dim)\n",
    "    \n",
    "    return X_train_data_, Y_train_data_, X_test_data_, Y_test_data_, X_train, Y_train, X_test, \\\n",
    "            Y_test, RNN_input_scaler, RNN_output_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN_model(idrop, rdrop, odrop, epochs, weight_decay, flag):\n",
    "    inputs = Input(shape=(window_size, feature_dim))\n",
    "\n",
    "    RNN_model = LSTM(50, return_sequences = True, activation = 'relu', kernel_regularizer = l2(weight_decay),\n",
    "                     recurrent_regularizer=l2(weight_decay), dropout=idrop, recurrent_dropout=rdrop)(inputs)\n",
    "    RNN_model = Dropout(odrop)(RNN_model)\n",
    "    \n",
    "    RNN_model = LSTM(50, return_sequences = False, activation = 'relu', kernel_regularizer = l2(weight_decay), \n",
    "                    recurrent_regularizer=l2(weight_decay), dropout=idrop, recurrent_dropout=rdrop)(RNN_model)\n",
    "    RNN_model = Dropout(odrop)(RNN_model)\n",
    "    \n",
    "    RNN_model = keras.layers.Reshape(target_shape = (1, 50))(RNN_model)\n",
    "    \n",
    "    RNN_model = Flatten()(RNN_model)\n",
    "\n",
    "    ## prediction\n",
    "    pred_speed = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_alt = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_x = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_y = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay),\n",
    "                            bias_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_lat = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay))(RNN_model)\n",
    "\n",
    "    pred_lng = Dense(units = step_ahead, activation='linear', kernel_regularizer=l2(weight_decay))(RNN_model)\n",
    "    \n",
    "    if flag:\n",
    "        RNN_model = Model(inputs=[inputs], outputs=[pred_alt, pred_x, pred_y])\n",
    "\n",
    "        RNN_model.compile(loss=\"mse\", optimizer=optimizers.adam(lr=0.001))\n",
    "        #RNN_model.summary()\n",
    "\n",
    "        RNN_hist = RNN_model.fit(X_train, [Y_train[:, :, 1], Y_train[:, :, 2], Y_train[:, :, 3]], \n",
    "                                 shuffle = True, verbose=0, epochs = epochs, batch_size = 2048)\n",
    "    else:\n",
    "        RNN_model = Model(inputs=[inputs], outputs=[pred_lat, pred_lng])\n",
    "\n",
    "        RNN_model.compile(loss=\"mse\", optimizer=optimizers.adam(lr=0.001))\n",
    "        #RNN_model.summary()\n",
    "\n",
    "        RNN_hist = RNN_model.fit(X_train, [Y_train[:, :, 4], Y_train[:, :, 5]], shuffle = True, \n",
    "                                 verbose=0, epochs = epochs, batch_size = 2048)\n",
    "    \n",
    "    return RNN_model, RNN_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the basic configuration of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "step_ahead = 5\n",
    "RNN_pred_steps = 2\n",
    "\n",
    "feature_size = window_size * feature_dim\n",
    "\n",
    "col_names = ['Speed', 'Altitude', 'x_velocity', 'y_velocity', 'Latitude', 'Longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of one-step LSTM prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RNN_one_step_position_pred():\n",
    "    plt.figure()\n",
    "    a = np.arange(col_names.index('Latitude'), feature_size, feature_dim)\n",
    "    b = np.arange(col_names.index('Longitude'), feature_size, feature_dim)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 6\n",
    "\n",
    "    legend_input = plt.scatter(X_test_data_.iloc[start_index, a].values, X_test_data_.iloc[start_index, b].values, \n",
    "                               s = 60, c = 'magenta', label = 'Input Data', marker = 'o')\n",
    "\n",
    "    for index_ in range(start_index, end_index):\n",
    "        legend_obs = plt.scatter(lat_test[index_, :], lng_test[index_, :], s = 80, c ='blue', \n",
    "                                 label ='Actual value', marker = 'x')\n",
    "\n",
    "        if index_ == start_index:\n",
    "            legend_pred = plt.scatter(RNN_lat_d[index_, :], RNN_lng_d[index_, :], s = 50, c= 'green', \n",
    "                                      marker = 's', label = 'Prediction')\n",
    "        else:\n",
    "            _ = plt.scatter(RNN_lat_d[index_, 4], RNN_lng_d[index_, 4], s = 50, marker = 's', c= 'green')\n",
    "\n",
    "    _ = plt.legend(handles = [legend_input, legend_obs, legend_pred], loc = 2)\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altitude Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_altitude_prediction():\n",
    "    \n",
    "    a = np.arange(col_names.index('Altitude'), feature_size, feature_dim)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 6\n",
    "\n",
    "    f = plt.figure(figsize = (12, 6))\n",
    "    legend_input = plt.scatter(range(window_size), X_test_data_.iloc[start_index, a].values, s = 60, \n",
    "                               c = 'magenta', label = 'Input Data', marker = 'o')\n",
    "\n",
    "    for index_ in range(start_index, end_index):\n",
    "        if index_ == start_index:\n",
    "            legend_obs = plt.scatter(range(window_size, window_size + step_ahead), \n",
    "                                 alt_test[index_, :], s = 80, c ='blue', label ='Actual Value', marker = 'x')\n",
    "\n",
    "            legend_pred = plt.scatter(range(window_size, window_size + step_ahead), \n",
    "                                      alt_model_pred[index_, :], s = 100, c= 'green', marker = '.', label = 'RNN Prediction')\n",
    "        else:\n",
    "            legend_obs = plt.scatter(range(window_size + step_ahead + index_-1, window_size + step_ahead+ index_), \n",
    "                                 alt_test[index_, 4], s = 80, c ='blue', label ='Actual Value', marker = 'x')\n",
    "\n",
    "            _ = plt.scatter(range(window_size + step_ahead + index_-1, window_size + step_ahead+ index_),\n",
    "                            alt_model_pred[index_, 4], s = 100, marker = '.', c= 'green')\n",
    "\n",
    "    plt.ylabel('Altitude (Unit: feet)', fontsize =16) \n",
    "    plt.xlabel('Time (Unit: 12 seconds)', fontsize =16)\n",
    "    _ = plt.legend(handles = [legend_input, legend_obs, legend_pred], loc = 2)\n",
    "    f.savefig('RNN_Alt.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN_altitude_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_pred_record(input_data, flag, step):\n",
    "    inversed_data = np.zeros((1, step_ahead, feature_dim))\n",
    "    input_data = input_data.reshape(-1, window_size, feature_dim)\n",
    "    \n",
    "    model_state_pred = np.array(state_RNN_model.predict(input_data))\n",
    "    \n",
    "    if step == 0:\n",
    "        model_lat_pred = RNN_lat_mu[row_index + step*step_ahead, :]\n",
    "        model_lng_pred = RNN_lng_mu[row_index + step*step_ahead, :]\n",
    "        model_alt_pred = RNN_alt_mu[row_index + step*step_ahead, :]\n",
    "    else:\n",
    "        Y_model_pred = np.array(lat_lng_RNN_model.predict(input_data))\n",
    "        model_lat_pred = inverse_RNN_pred(col_names.index('Latitude'), Y_model_pred[0, :, :])\n",
    "        model_lng_pred = inverse_RNN_pred(col_names.index('Longitude'), Y_model_pred[1, :, :])\n",
    "        model_alt_pred = inverse_RNN_pred(col_names.index('Altitude'), model_state_pred[0, :, :])\n",
    "\n",
    "    model_x_velocity_pred = inverse_RNN_pred(col_names.index('x_velocity'), model_state_pred[1, :, :])\n",
    "    model_y_velocity_pred = inverse_RNN_pred(col_names.index('y_velocity'), model_state_pred[2, :, :])\n",
    "        \n",
    "    if flag:\n",
    "        model_lat_pred = model_lat_pred + DNN_RNN_correction[0] #+ np.arange(1, 6) * DNN_actual_correction[0]\n",
    "        model_lng_pred = model_lng_pred + DNN_RNN_correction[1] #+ np.arange(1, 6) * DNN_actual_correction[1]\n",
    "        model_alt_pred = model_alt_pred #+ DNN_RNN_correction[2]\n",
    "  \n",
    "    speed = np.sqrt(np.square(model_x_velocity_pred) + np.square(model_y_velocity_pred))\n",
    "    inversed_data[:, :, 0] = speed\n",
    "    inversed_data[:, :, 1] = model_alt_pred\n",
    "    inversed_data[:, :, 2] = model_x_velocity_pred\n",
    "    inversed_data[:, :, 3] = model_y_velocity_pred\n",
    "    inversed_data[:, :, 4] = model_lat_pred\n",
    "    inversed_data[:, :, 5] = model_lng_pred\n",
    "    \n",
    "    return inversed_data.reshape(step_ahead, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_RNN_pred(row_index, flag = True):\n",
    "    RNN_multi_pred = np.zeros((RNN_pred_steps, step_ahead, feature_dim))\n",
    "    \n",
    "    for i in range(RNN_pred_steps):\n",
    "        input_data = copy.deepcopy(X_test_data_.iloc[row_index + i*step_ahead].values)\n",
    "        new_input_data = input_data.reshape(1, window_size * feature_dim)\n",
    "        \n",
    "        if i == 0:\n",
    "            RNN_pred_tm_ = create_RNN_pred_record(RNN_input_scaler.transform(new_input_data), flag, i)\n",
    "        else:\n",
    "            new_input_data[0, (window_size - step_ahead)*feature_dim:window_size*feature_dim] = \\\n",
    "                                        RNN_pred_tm_.reshape(step_ahead * feature_dim, )\n",
    "            \n",
    "            RNN_pred_tm_ = create_RNN_pred_record(RNN_input_scaler.transform(new_input_data), flag, i)\n",
    "        \n",
    "        RNN_multi_pred[i, :, :] = RNN_pred_tm_\n",
    "    \n",
    "    return RNN_multi_pred.reshape(RNN_pred_steps*step_ahead, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrepancy between DNN and RNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discrepancy(row_index):\n",
    "    DNN_RNN_correction = list()\n",
    "    DNN_actual_correction = list()\n",
    "\n",
    "    DNN_index_ = row_index + window_size - 1\n",
    "    \n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~Deviation between DNN and RNN ~~~~~~~~~~~~~')\n",
    "#     print ('Lat true value: ', Y_lat_true[DNN_index_], 'Lng true value: ', Y_lng_true[DNN_index_])\n",
    "#     print ('Lat DNN mean value: ', DNN_lat_mu[DNN_index_], 'Lng DNN mean value: ', DNN_lng_mu[DNN_index_])\n",
    "\n",
    "#     print ('Lat RNN mean value:', RNN_lat_mu[row_index, 0], 'Lng RNN mean value:', RNN_lng_mu[row_index, 0])\n",
    "\n",
    "    lat_dev__ = (DNN_lat_mu[DNN_index_] - RNN_lat_mu[row_index, 0])\n",
    "    lng_dev__ = (DNN_lng_mu[DNN_index_] - RNN_lng_mu[row_index, 0])\n",
    "    alt_dev__ = (DNN_alt_mu[DNN_index_] - RNN_alt_mu[row_index, 0])\n",
    "    DNN_RNN_correction.append([lat_dev__, lng_dev__, alt_dev__])\n",
    "\n",
    "#    print ('Dev value: ', lat_dev__, 'Lng: ', lng_dev__)\n",
    "    \n",
    "    DNN_actual_lat_dev = 0\n",
    "    DNN_actual_lng_dev = 0\n",
    "    \n",
    "    for j in np.arange(1, 6):\n",
    "        DNN_actual_lat_dev += (Y_lat_true[DNN_index_ - j] - DNN_lat_mu[DNN_index_ - j])\n",
    "        DNN_actual_lng_dev += (Y_lng_true[DNN_index_ - j] - DNN_lng_mu[DNN_index_ - j])\n",
    "        \n",
    "    DNN_actual_lat_dev /= 5\n",
    "    DNN_actual_lng_dev /= 5\n",
    "    #print (DNN_actual_lat_dev, DNN_actual_lng_dev)\n",
    "    \n",
    "    DNN_actual_correction.append([DNN_actual_lat_dev, DNN_actual_lng_dev])\n",
    "    \n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~Deviation between actual and DNN ~~~~~~~~~~~~~')\n",
    "#     print ('Actual value:', actual_lat[DNN_index_ - 1], actual_lng[DNN_index_ - 1])\n",
    "#     print ('DNN prediction mean: ', DNN_lat_mu[DNN_index_ - 1], DNN_lng_mu[DNN_index_ - 1])\n",
    "#     print ('Dev value:', actual_lat[DNN_index_ - 1] - DNN_lat_mu[DNN_index_ - 1], \n",
    "#        actual_lng[DNN_index_ - 1] - DNN_lng_mu[DNN_index_ - 1])\n",
    "    \n",
    "#     print ('\\n')\n",
    "#     print ('~~~~~~~~~~~~~~~~~~~~~~~~~After correction ~~~~~~~~~~~~~')\n",
    "#     print ('After correction: Lat value:', RNN_lat_mu[row_index, 0] + lat_dev__, \n",
    "#           'Lng value:', RNN_lng_mu[row_index, 0] + lng_dev__)\n",
    "#     print ('After correction: Lat value:', RNN_lat_mu[row_index, 0] + lat_dev__ + DNN_actual_lat_dev, \n",
    "#           'Lng value:', RNN_lng_mu[row_index, 0] + lng_dev__ + DNN_actual_lng_dev)\n",
    "\n",
    "    return np.array(DNN_RNN_correction).T, np.array(DNN_actual_correction).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A demo on Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lng_model_comparsion():\n",
    "    \n",
    "    f = plt.figure(figsize = (12, 6))\n",
    "    a = np.arange(4, feature_size, feature_dim)\n",
    "    b = np.arange(5, feature_size, feature_dim)\n",
    "\n",
    "    #input_plt = plt.scatter(X_test_data_.iloc[row_index, a].values, X_test_data_.iloc[row_index, b].values, \n",
    "    #                               s = 80, c = 'magenta', label = 'Input Data', marker = 'X')\n",
    "\n",
    "    In_val = plt.scatter(multi_pred_with_DNN[:, 5], multi_pred_with_DNN[:, 4], s = 80, c= 'blue', label = 'Integrated Model')\n",
    "    RNN_only_ = plt.scatter(multi_pred_no_DNN[:, 5], multi_pred_no_DNN[:, 4], s = 80, c= 'green', label = 'RNN Prediction')\n",
    "    Actual_val = plt.scatter(actual_val[s:s+10, 5], actual_val[s:s+10, 4], marker = '+', c = 'red', s = 90,\n",
    "                             label = 'Actual Value')\n",
    "\n",
    "    _ = plt.legend(handles = [In_val, RNN_only_, Actual_val])\n",
    "    _ = plt.xlabel('Longitude')\n",
    "    _ = plt.ylabel('Latitude')\n",
    "    #plt.ylim([37.125, 37.325])\n",
    "    \n",
    "    \n",
    "    f.savefig('RNN_Lat_Lng_comp.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def altitude_pred_comparison():\n",
    "    a = np.arange(4, feature_size, feature_dim)\n",
    "    b = np.arange(5, feature_size, feature_dim)\n",
    "\n",
    "    f = plt.figure()\n",
    "    In_val = plt.scatter(np.arange(1, 11), multi_pred_with_DNN[:, 1], s = 80, c= 'blue', label = 'Integrated Model')\n",
    "    RNN_only_ = plt.scatter(np.arange(1, 11), multi_pred_no_DNN[:, 1], s = 80, c= 'green', label = 'RNN Only')\n",
    "    Actual_val = plt.scatter(np.arange(1, 11), actual_val[s:s+10, 1], marker = '+', c = 'red', s = 90,\n",
    "                             label = 'Actual Value')\n",
    "\n",
    "    _ = plt.legend(handles = [In_val, RNN_only_, Actual_val])\n",
    "    _ = plt.xlabel('Iterations')\n",
    "    _ = plt.ylabel('Altitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of latitude and longitude deviation of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_deviation_scatter_plot():\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale_view()\n",
    "    \n",
    "    integrated_dev_plt = plt.scatter(integrated_lat_dev, integrated_lng_dev, s = 20, c = 'magenta', alpha = 0.3, \n",
    "                                     label = 'Deviation between IP and AT')\n",
    "\n",
    "    no_DNN_dev = plt.scatter(no_DNN_lat_dev, no_DNN_lng_dev, s = 20, c = 'blue', alpha = 0.15, \n",
    "                             label = 'Deviation between RP and AT')\n",
    "\n",
    "    plt.xlabel('Latitude deviation')\n",
    "    plt.ylabel('Longitude deviation')\n",
    "    _ = plt.legend(handles = [integrated_dev_plt, no_DNN_dev], loc = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caldulate Uncertainty Reduction Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_uncertainty_reduction_visualization():\n",
    "    #############################\n",
    "    #############################\n",
    "    n_groups = 2\n",
    "\n",
    "    # create plot\n",
    "    f = plt.figure(figsize=(14, 7))\n",
    "    f.add_subplot(121)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, mu_no_DNN, bar_width, alpha=opacity, color='b', label='RNN Model')\n",
    "    rects2 = plt.bar(index + bar_width, mu_integrated, bar_width, alpha=opacity, color='g', label='Integrated Model')\n",
    "\n",
    "    plt.ylabel(r'$\\mu$ of trajectory deviation from AT')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    #############################\n",
    "    f.add_subplot(122)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, sd_no_DNN, bar_width, alpha=opacity, color='b', label='RNN Model')\n",
    "    rects2 = plt.bar(index + bar_width, sd_integrated, bar_width, alpha=opacity, color='g', label='Integrated Model')\n",
    "\n",
    "    plt.ylabel(r'$\\sigma$ of trajectory deviation from AT')\n",
    "    plt.xticks([0.17, 1.17], ('Latitude', 'Longitude'))\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute spatial distance from latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    radius = 3958 # unit in miles\n",
    "\n",
    "    dLat = (lat2-lat1) * math.pi / 180\n",
    "    dLng = (lon2-lon1) * math.pi / 180\n",
    "\n",
    "    lat1 = lat1 * math.pi / 180\n",
    "    lat2 = lat2 * math.pi / 180\n",
    "\n",
    "    val = sin(dLat/2) * sin(dLat/2) + sin(dLng/2) * sin(dLng/2) * cos(lat1) * cos(lat2)    \n",
    "    ang = 2 * atan2(sqrt(val), sqrt(1-val))\n",
    "    return radius * ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Speed', 'Altitude', 'x_velocity', 'y_velocity', 'Latitude', 'Longitude']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_loop = 1\n",
    "inner_loop = 1\n",
    "\n",
    "DNN_loss = list()\n",
    "DNN_reduction = list()\n",
    "\n",
    "RNN_loss = list()\n",
    "RNN_reduction = list()\n",
    "correction = list()\n",
    "\n",
    "n_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Iter:  0 Inner Iter:  0\n",
      "2019-01-13\n",
      "AAL598-lat_dev_2019-01-13.pth\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "## randomly select a trajectory to test the performance of the algorithm\n",
    "for outer_iter in range(outer_loop):\n",
    "    random_order = shuffle(np.arange(len(interpolated_df)), random_state = outer_iter + 1)\n",
    "    \n",
    "    for inner_iter in range(inner_loop):\n",
    "        print ('Outer Iter: ', outer_iter, 'Inner Iter: ', inner_iter)\n",
    "        \n",
    "        ## Determine test flight trajectory\n",
    "        test_index = random_order[inner_iter]\n",
    "        test_rec_date = str(interpolated_df[test_index]['actualPositionTime'].iloc[0])[:10]\n",
    "        test_rec_date = test_rec_date.strip()\n",
    "        print (test_rec_date)\n",
    "        \n",
    "        #################################################################################\n",
    "        ################################## DNN model for latitude #######################\n",
    "        #################################################################################\n",
    "        lat_col_name = ['lat_dev']\n",
    "        lat_model_name = flight_ID + '-' + ''.join(lat_col_name) + '_' + test_rec_date + '.pth'\n",
    "\n",
    "        DNN_X_lat_train, DNN_Y_lat_train, X_train_lat, Y_train_lat, X_test_lat, Y_test_lat,\\\n",
    "                                    lat_input_scaler, lat_output_scaler = prepare_data(lat_col_name)\n",
    "\n",
    "        print (lat_model_name)\n",
    "        \n",
    "        if os.path.exists(lat_model_name) and use_trained_model:\n",
    "            lat_model = torch.load(lat_model_name)\n",
    "        else:\n",
    "            lat_model = PyTorch_build_DNN_model(X_train_lat, Y_train_lat)\n",
    "            torch.save(lat_model, lat_model_name)\n",
    "\n",
    "        ########### Deterministic DNN Prediction for Latitude ##################    \n",
    "        Y_lat_deter_pred = PyTorch_DNN_deterministic_pred(lat_model, X_test_lat)\n",
    "        Y_lat_dev_pred = transform(lat_output_scaler, Y_lat_deter_pred)\n",
    "        Y_lat_pred = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_dev_pred)\n",
    "        Y_lat_true = convert_to_actual_val(col_name = 'actual_Latitude')\n",
    "\n",
    "        ########### Probabilistic DNN Prediction for Latitude ##################\n",
    "        y_pred_do_lat, y_pred_do_mean_lat = PyTorch_DNN_probabilistic_pred(lat_model, X_test_lat, n_iter)\n",
    "        Y_lat_dev_pred_mean = transform(lat_output_scaler, y_pred_do_mean_lat)\n",
    "        Y_lat_pred_mean_ = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_dev_pred_mean)\n",
    "        \n",
    "        ########################################################################################\n",
    "        ################################## DNN model for longitude #############################\n",
    "        ########################################################################################\n",
    "        lng_col_name = ['lng_dev']\n",
    "        lng_model_name = flight_ID + '-' + ''.join(lng_col_name) + '_' + test_rec_date + '.pth'\n",
    "\n",
    "        DNN_X_lng_train, DNN_Y_lng_train, X_train_lng, Y_train_lng, X_test_lng, Y_test_lng, \\\n",
    "                                    lng_input_scaler, lng_output_scaler = prepare_data(lng_col_name)\n",
    "        \n",
    "        if os.path.exists(lng_model_name) and use_trained_model:\n",
    "            lng_model = torch.load(lng_model_name)\n",
    "        else:\n",
    "            lng_model = PyTorch_build_DNN_model(X_train_lng, Y_train_lng)\n",
    "            torch.save(lng_model, lng_model_name)\n",
    "\n",
    "        ########### Deterministic DNN Prediction for Longitude ################## \n",
    "        Y_lng_deter_pred = PyTorch_DNN_deterministic_pred(lng_model, X_test_lng)\n",
    "        Y_lng_dev_pred = transform(lng_output_scaler, Y_lng_deter_pred)\n",
    "        Y_lng_pred = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_dev_pred)\n",
    "        Y_lng_true = convert_to_actual_val(col_name = 'actual_Longitude')\n",
    "\n",
    "        ### altitude\n",
    "        Y_alt_pred = convert_to_actual_val(col_name = 'targetAltitude')\n",
    "\n",
    "        ########### Probabilistic DNN Prediction for Longitude ##################\n",
    "        y_pred_do_lng, y_pred_do_mean_lng = PyTorch_DNN_probabilistic_pred(lng_model, X_test_lng, n_iter)\n",
    "        Y_lng_dev_pred_mean = transform(lng_output_scaler, y_pred_do_mean_lng)\n",
    "        Y_lng_pred_mean_ = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_dev_pred_mean)\n",
    "        \n",
    "        lat_dev_baseline, lng_dev_baseline, lat_dev_actual_pred, lng_dev_actual_pred, lat_mu_reduction, lng_mu_reduction, \\\n",
    "                        lat_sd_reduction, lng_sd_reduction = DNN_calculate_deviation()\n",
    "        \n",
    "        #DNN_combined_results()\n",
    "        \n",
    "        ###################  DNN Loss   #########################\n",
    "        lat_deter_loss_mse_d = mean_squared_error(Y_lat_true, Y_lat_pred)\n",
    "        lng_deter_loss_mse_d = mean_squared_error(Y_lng_true, Y_lng_pred)\n",
    "        lat_loss_mse_p = mean_squared_error(Y_lat_true, Y_lat_pred_mean_)\n",
    "        lng_loss_mse_p = mean_squared_error(Y_lng_true, Y_lng_pred_mean_)\n",
    "        \n",
    "        lat_deter_loss_mae_d = mean_absolute_error(Y_lat_true, Y_lat_pred)\n",
    "        lng_deter_loss_mae_d = mean_absolute_error(Y_lng_true, Y_lng_pred)\n",
    "        lat_loss_mae_p = mean_absolute_error(Y_lat_true, Y_lat_pred_mean_)\n",
    "        lng_loss_mae_p = mean_absolute_error(Y_lng_true, Y_lng_pred_mean_)\n",
    "        \n",
    "        # calculate spatial distance between prediction and actual values\n",
    "        spatial_deter_dist = []\n",
    "        spatial_prob_dist = []\n",
    "        for i in range(Y_lat_pred.shape[0]):\n",
    "            lat1, lon1, lat2, lon2 = Y_lat_pred[i], Y_lng_pred[i], Y_lat_true[i], Y_lng_true[i]\n",
    "            spatial_deter_dist.append(get_distance(lat1, lon1, lat2, lon2))\n",
    "            \n",
    "            lat1, lon1, lat2, lon2 = Y_lat_pred_mean_[i], Y_lng_pred_mean_[i], Y_lat_true[i], Y_lng_true[i]\n",
    "            spatial_prob_dist.append(get_distance(lat1, lon1, lat2, lon2))\n",
    "            \n",
    "        spatial_deter_loss = np.mean(spatial_deter_dist)\n",
    "        spatial_prob_dist = np.mean(spatial_prob_dist)\n",
    "        \n",
    "        DNN_loss.append([lat_deter_loss_mse_d, lng_deter_loss_mse_d, lat_loss_mse_p, lng_loss_mse_p,\n",
    "                         lat_deter_loss_mae_d, lng_deter_loss_mae_d, lat_loss_mae_p, lng_loss_mae_p,\n",
    "                         spatial_deter_loss, spatial_prob_dist])\n",
    "        \n",
    "        DNN_reduction.append([lat_mu_reduction, lat_sd_reduction, lng_mu_reduction, lng_sd_reduction])\n",
    "        \n",
    "        ##############################################################################\n",
    "        ########################     RNN Lat Lng Model Construction   ################\n",
    "        ##############################################################################\n",
    "        lat_lng_RNN_model_name = flight_ID + '-' + test_rec_date + '-' + 'lat_lng_RNN_model.h5'\n",
    "\n",
    "        X_train_data_, Y_train_data_, X_test_data_, Y_test_data_, X_train, Y_train, X_test, \\\n",
    "            Y_test, RNN_input_scaler, RNN_output_scaler = generate_RNN_data(window_size, step_ahead)\n",
    "\n",
    "        if os.path.exists(lat_lng_RNN_model_name) and use_trained_model:\n",
    "            lat_lng_RNN_model = load_model(lat_lng_RNN_model_name)\n",
    "        else:\n",
    "            ### Latitude & longitude model\n",
    "            lat_lng_RNN_model, RNN_hist = build_RNN_model(idrop = 0, rdrop = 0.1, odrop = 0.1, epochs = 1500,\n",
    "                                                          weight_decay = 1e-5, flag = False)\n",
    "            lat_lng_RNN_model.save(lat_lng_RNN_model_name)\n",
    "        \n",
    "        \n",
    "        ############################## RNN Prediction on Position ##################\n",
    "        no_output = 2\n",
    "\n",
    "        # four dimensions in result: no of samples, no of model output, no of rows in input_data, step_ahead\n",
    "        RNN_pos_p, RNN_pos_p_mu, RNN_pos_d = RNN_probabilistic_prediction(lat_lng_RNN_model, X_test, no_output, n_iter)\n",
    "            \n",
    "        #### Latitude\n",
    "        loc = col_names.index('Latitude')\n",
    "        RNN_lat_d, lat_test = RNN_inverse_transform(loc, RNN_pos_d[0, :, ], Y_test)\n",
    "        #print ('Latitude: ', mean_squared_error(RNN_lat_d, lat_test))\n",
    "\n",
    "        #### Longitude\n",
    "        loc = col_names.index('Longitude')\n",
    "        RNN_lng_d, lng_test = RNN_inverse_transform(loc, RNN_pos_d[1, :, ], Y_test)\n",
    "        #print ('Longitude:', mean_squared_error(RNN_lng_d, lng_test))\n",
    "        \n",
    "        #RNN_one_step_position_pred()\n",
    "        \n",
    "        \n",
    "        #######################################################################\n",
    "        ##################       RNN State Model Construction   ###############\n",
    "        #######################################################################\n",
    "        state_RNN_model_name = flight_ID + '-' + test_rec_date + '-' + 'state_RNN_model.h5'\n",
    "\n",
    "        if os.path.exists(state_RNN_model_name) and use_trained_model:\n",
    "            state_RNN_model = load_model(state_RNN_model_name)\n",
    "        else:\n",
    "            state_RNN_model, RNN_hist = build_RNN_model(idrop = 0, rdrop = 0.1, odrop = 0.1, epochs=1500, \n",
    "                                                        weight_decay = 6e-5, flag = True)\n",
    "            state_RNN_model.save(state_RNN_model_name)\n",
    "            \n",
    "            \n",
    "        ############################################ RNN Prediction on State ####################################\n",
    "        no_output = 3\n",
    "\n",
    "        # four dimensions in result: no of samples, no of model output, no of rows in input_data, step_ahead\n",
    "        RNN_state_p, RNN_state_p_mu, RNN_state_d = RNN_probabilistic_prediction(state_RNN_model, X_test, no_output, n_iter)\n",
    "    \n",
    "        loc = col_names.index('Altitude')\n",
    "        alt_model_pred, alt_test = RNN_inverse_transform(loc, RNN_state_d[0, :, ], Y_test)\n",
    "        #print ('Altitude: ', mean_squared_error(alt_model_pred, alt_test))\n",
    "\n",
    "        loc = col_names.index('x_velocity')\n",
    "        x_model_pred, x_vel_test = RNN_inverse_transform(loc, RNN_state_d[1, :, ], Y_test)\n",
    "        #print ('x_velocity: ', mean_squared_error(x_model_pred, x_vel_test))\n",
    "\n",
    "        loc = col_names.index('y_velocity')\n",
    "        y_model_pred, y_vel_test = RNN_inverse_transform(loc, RNN_state_d[2, :, ], Y_test)\n",
    "        #print ('y_velocity: ', mean_squared_error(y_model_pred, y_vel_test))\n",
    "    \n",
    "        \n",
    "        actual_val = np.array(Y_test_data_)\n",
    "        \n",
    "        ######################################### Uncertainty Reduction in Integrated Model #########################\n",
    "        integrated_lat_dev = list()\n",
    "        integrated_lng_dev = list()\n",
    "        integrated_alt_dev = list()\n",
    "        no_DNN_lat_dev = list()\n",
    "        no_DNN_lng_dev = list()\n",
    "        no_DNN_alt_dev = list()\n",
    "        \n",
    "        #Y_test.shape[0] - step_ahead * RNN_pred_steps\n",
    "        \n",
    "        for s in range(Y_test.shape[0] - 20 * step_ahead * RNN_pred_steps):\n",
    "            #print ('shape', Y_test.shape[0])\n",
    "            #print (s)\n",
    "            row_index = s\n",
    "\n",
    "            DNN_index = row_index + window_size - 1\n",
    "\n",
    "            Y_lat_pred_ = transform(lat_output_scaler, y_pred_do_mean_lat[DNN_index])\n",
    "            Y_lng_pred_ = transform(lng_output_scaler, y_pred_do_mean_lng[DNN_index])\n",
    "\n",
    "            DNN_lat_mu = convert_to_actual_val(col_name = 'target_Latitude', dev_pred = Y_lat_pred_)\n",
    "            DNN_lng_mu = convert_to_actual_val(col_name = 'target_Longitude', dev_pred = Y_lng_pred_)\n",
    "            DNN_alt_mu = Y_alt_pred\n",
    "\n",
    "            ############################### RNN Prediciton  #########################\n",
    "\n",
    "            #### Latitude\n",
    "            loc = col_names.index('Latitude')\n",
    "            RNN_lat_mu = inverse_RNN_pred(loc, RNN_pos_p_mu[0, :, ])\n",
    "\n",
    "            #### Longitude\n",
    "            loc = col_names.index('Longitude')\n",
    "            RNN_lng_mu = inverse_RNN_pred(loc, RNN_pos_p_mu[1, :, ])\n",
    "\n",
    "            #### Altitude\n",
    "            loc = col_names.index('Altitude')\n",
    "            RNN_alt_mu = inverse_RNN_pred(loc, RNN_state_p_mu[0, :, ])\n",
    "\n",
    "            #### X_velocity\n",
    "            loc = col_names.index('x_velocity')\n",
    "            RNN_x_velocity_mu = inverse_RNN_pred(loc, RNN_state_p_mu[1, :, ])\n",
    "\n",
    "            #### Y_velocity\n",
    "            loc = col_names.index('y_velocity')\n",
    "            RNN_y_velocity_mu = inverse_RNN_pred(loc, RNN_state_p_mu[2, :, ])\n",
    "\n",
    "            ########################## Calculate discrepancy  #####################\n",
    "            DNN_RNN_correction, DNN_actual_correction = calculate_discrepancy(row_index)\n",
    "            multi_pred_with_DNN = multi_step_RNN_pred(row_index, flag = True)\n",
    "            multi_pred_no_DNN = multi_step_RNN_pred(row_index, flag = False)\n",
    "            correction.append(DNN_RNN_correction)\n",
    "    \n",
    "            ########################## Record Deviation ##########################\n",
    "            integrated_lat_dev.append(np.abs(multi_pred_with_DNN[:, 4] - actual_val[s:s+10, 4]))\n",
    "            integrated_lng_dev.append(np.abs(multi_pred_with_DNN[:, 5] - actual_val[s:s+10, 5]))\n",
    "            integrated_alt_dev.append(np.abs(multi_pred_with_DNN[:, 1] - actual_val[s:s+10, 1]))\n",
    "            \n",
    "\n",
    "            no_DNN_lat_dev.append(np.abs(multi_pred_no_DNN[:, 4] - actual_val[s:s+10, 4]))\n",
    "            no_DNN_lng_dev.append(np.abs(multi_pred_no_DNN[:, 5] - actual_val[s:s+10, 5]))\n",
    "            no_DNN_alt_dev.append(np.abs(multi_pred_no_DNN[:, 1] - actual_val[s:s+10, 1]))\n",
    "        \n",
    "        integrated_lat_dev = np.concatenate(integrated_lat_dev, axis=0)\n",
    "        integrated_lng_dev = np.concatenate(integrated_lng_dev, axis=0)\n",
    "        integrated_alt_dev = np.concatenate(integrated_alt_dev, axis=0)\n",
    "        \n",
    "        no_DNN_lat_dev = np.concatenate(no_DNN_lat_dev, axis=0)\n",
    "        no_DNN_lng_dev = np.concatenate(no_DNN_lng_dev, axis=0)\n",
    "        no_DNN_alt_dev = np.concatenate(no_DNN_alt_dev, axis=0)\n",
    "        \n",
    "        mu_int_lat_dev = np.mean(integrated_lat_dev)\n",
    "        sd_int_lat_dev = np.std(integrated_lat_dev)\n",
    "\n",
    "        mu_int_lng_dev = np.mean(integrated_lng_dev)\n",
    "        sd_int_lng_dev = np.std(integrated_lng_dev)\n",
    "\n",
    "        mu_no_DNN_lat_dev = np.mean(no_DNN_lat_dev)\n",
    "        sd_no_DNN_lat_dev = np.std(no_DNN_lat_dev)\n",
    "\n",
    "        mu_no_DNN_lng_dev = np.mean(no_DNN_lng_dev)\n",
    "        sd_no_DNN_lng_dev = np.std(no_DNN_lng_dev)\n",
    "\n",
    "        mu_integrated = (mu_int_lat_dev, mu_int_lng_dev)\n",
    "        mu_no_DNN = (mu_no_DNN_lat_dev, mu_no_DNN_lng_dev)\n",
    "\n",
    "        sd_integrated = (sd_int_lat_dev, sd_int_lng_dev)\n",
    "        sd_no_DNN = (sd_no_DNN_lat_dev, sd_no_DNN_lng_dev)\n",
    "\n",
    "        percent_mu_lat = (mu_no_DNN_lat_dev - mu_int_lat_dev)/mu_no_DNN_lat_dev\n",
    "        percent_mu_lng = (mu_no_DNN_lng_dev - mu_int_lng_dev)/mu_no_DNN_lng_dev\n",
    "\n",
    "        percent_sd_lat = (sd_no_DNN_lat_dev - sd_int_lat_dev)/sd_no_DNN_lat_dev\n",
    "        percent_sd_lng = (sd_no_DNN_lng_dev - sd_int_lng_dev)/sd_no_DNN_lng_dev\n",
    "        \n",
    "        \n",
    "        #####################################  RNN Loss #######################################\n",
    "        RNN_lat_loss_mse = np.sqrt(np.mean(np.array(no_DNN_lat_dev)**2))\n",
    "        RNN_lng_loss_mse = np.sqrt(np.mean(np.array(no_DNN_lng_dev)**2))\n",
    "        Integrated_lat_loss_mse = np.sqrt(np.mean(np.array(integrated_lat_dev)**2))\n",
    "        Integrated_lng_loss_mse = np.sqrt(np.mean(np.array(integrated_lng_dev)**2))\n",
    "        RNN_alt_loss_mse = np.sqrt(np.mean(np.array(no_DNN_alt_dev)**2))\n",
    "        x_velocity_loss_mse = mean_squared_error(x_vel_test, x_model_pred)\n",
    "        y_velocity_loss_mse = mean_squared_error(y_vel_test, y_model_pred)\n",
    "        \n",
    "        \n",
    "        RNN_lat_loss_mae = np.mean(no_DNN_lat_dev)\n",
    "        RNN_lng_loss_mae = np.mean(no_DNN_lng_dev)\n",
    "        Integrated_lat_loss_mae = np.mean(integrated_lat_dev)\n",
    "        Integrated_lng_loss_mae = np.mean(integrated_lng_dev)\n",
    "        RNN_alt_loss_mae = np.mean(no_DNN_alt_dev)\n",
    "        x_velocity_loss_mae = mean_absolute_error(x_vel_test, x_model_pred)\n",
    "        y_velocity_loss_mae = mean_absolute_error(y_vel_test, y_model_pred)\n",
    "        \n",
    "        \n",
    "        RNN_loss.append([RNN_lat_loss_mse, RNN_lng_loss_mse, Integrated_lat_loss_mse, Integrated_lng_loss_mse, RNN_alt_loss_mse, \n",
    "                         x_velocity_loss_mse, y_velocity_loss_mse,\n",
    "                         RNN_lat_loss_mae, RNN_lng_loss_mae, Integrated_lat_loss_mae, Integrated_lng_loss_mae, RNN_alt_loss_mae,\n",
    "                         x_velocity_loss_mae, y_velocity_loss_mae])\n",
    "            \n",
    "        RNN_reduction.append([percent_mu_lat, percent_mu_lng, percent_sd_lat, percent_sd_lng])\n",
    "\n",
    "\n",
    "    #RNN_altitude_prediction()\n",
    "    #altitude_pred_comparison()\n",
    "    #lat_lng_model_comparsion()\n",
    "    #RNN_deviation_scatter_plot()\n",
    "    #RNN_uncertainty_reduction_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.298333, -79.174444],\n",
       "       [ 37.284722, -79.191944],\n",
       "       [ 37.268056, -79.215   ],\n",
       "       [ 37.251389, -79.233611],\n",
       "       [ 37.233889, -79.251667],\n",
       "       [ 37.215833, -79.268611],\n",
       "       [ 37.1975  , -79.284167],\n",
       "       [ 37.179167, -79.299722],\n",
       "       [ 37.161111, -79.315   ],\n",
       "       [ 37.142222, -79.330556]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_val[s:s+10, 4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37.2972466 , -79.17536684],\n",
       "       [ 37.28049128, -79.19280532],\n",
       "       [ 37.2636337 , -79.21014362],\n",
       "       [ 37.24669706, -79.22737607],\n",
       "       [ 37.22967345, -79.24451564],\n",
       "       [ 37.21827412, -79.26738677],\n",
       "       [ 37.20159144, -79.28452122],\n",
       "       [ 37.18478008, -79.30153674],\n",
       "       [ 37.16786579, -79.31842755],\n",
       "       [ 37.15084027, -79.33520869]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_pred_with_DNN[:, 4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_lng_model_comparsion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021386198387454057"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(multi_pred_no_DNN[:, 4:6], actual_val[s:s+10, 4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038832929864856423"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(multi_pred_with_DNN[:, 4:6], actual_val[s:s+10, 4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(flight_ID + '-' + 'results.pkl', 'wb') as f:\n",
    "#      pickle.dump([DNN_loss, DNN_reduction, RNN_loss, RNN_reduction], f)\n",
    "    \n",
    "# with open(flight_ID + '-' + 'correction.pkl', 'wb') as f:\n",
    "#     pickle.dump(correction, f)\n",
    "    \n",
    "# with open(flight_ID + '-' + 'pro_prediction.pkl', 'wb') as f:\n",
    "#     pickle.dump([RNN_pos_p, RNN_state_p], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================  ....DNN model.... =========================\n",
      "====================  Root mean squared error =========================\n",
      "DNN deterministic model: Lat -->  0.0008467573550025812 ; Lng -->  0.0009280279713061197\n",
      "DNN probabilistic model: Lat -->  0.0008288327311803689 ; Lng -->  0.0009003515710096197\n",
      "====================  Mean absolute error =========================\n",
      "DNN deterministic model: Lat -->  0.0006057260326899916 ; Lng -->  0.0006484171196048398\n",
      "DNN probabilistic model: Lat -->  0.0005823370089485561 ; Lng -->  0.0006172760901735971\n",
      "====================  Spatial distance    =========================\n",
      "DNN deterministic model: spatial dist -->  0.059294232731700376 ; probabilistic model spatial dist -->  0.05679761355560349\n",
      "\n",
      "\n",
      "====================  .... LSTM model .... =========================\n",
      "====================  Root mean squared error =========================\n",
      "RNN LSTM       model: Lat -->  0.029292595732942864 ; Lng -->  0.034202973168735155 ; Alt -->  720.3683895831624 ; X velocity -->  17.936669300991507 ; Y velocity -->  18.926130102751085\n",
      "RNN integrated model: Lat -->  0.014794298621685078 ; Lng -->  0.014446565705243837\n",
      "====================  Mean absolute error =========================\n",
      "RNN LSTM       model: Lat -->  0.0226591121924432 ; Lng -->  0.026367889967395284 ; Alt -->  529.8462507264944 ; X velocity -->  8.329780468244342 ; Y velocity -->  9.080826066800833\n",
      "RNN integrated model: Lat -->  0.009611645345020977 ; Lng -->  0.00928578034162216\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "AA598_res = open(flight_ID + '-results.pkl', 'rb')\n",
    "res = pickle.load(AA598_res)\n",
    "\n",
    "DNN_loss = np.array(res[0])\n",
    "\n",
    "print ('====================  ....DNN model.... =========================')\n",
    "print ('====================  Root mean squared error =========================')\n",
    "print ('DNN deterministic model: Lat --> ', np.mean(np.sqrt(DNN_loss[:, 0])), '; Lng --> ', np.mean(np.sqrt(DNN_loss[:, 1])))\n",
    "print ('DNN probabilistic model: Lat --> ', np.mean(np.sqrt(DNN_loss[:, 2])), '; Lng --> ', np.mean(np.sqrt(DNN_loss[:, 3])))\n",
    "\n",
    "print ('====================  Mean absolute error =========================')\n",
    "print ('DNN deterministic model: Lat --> ', np.mean(DNN_loss[:, 4]), '; Lng --> ', np.mean(DNN_loss[:, 5]))\n",
    "print ('DNN probabilistic model: Lat --> ', np.mean(DNN_loss[:, 6]), '; Lng --> ', np.mean(DNN_loss[:, 7]))\n",
    "\n",
    "print ('====================  Spatial distance    =========================')\n",
    "print ('DNN deterministic model: spatial dist --> ', np.mean(DNN_loss[:, 8]), \n",
    "       '; probabilistic model spatial dist --> ', np.mean(DNN_loss[:, 9]))\n",
    "print ('\\n')\n",
    "\n",
    "\n",
    "RNN_loss = np.array(res[2])\n",
    "\n",
    "print ('====================  .... LSTM model .... =========================')\n",
    "print ('====================  Root mean squared error =========================')\n",
    "print ('RNN LSTM       model: Lat --> ', np.mean(RNN_loss[:, 0]), '; Lng --> ', np.mean(RNN_loss[:, 1]), \\\n",
    "                    '; Alt --> ', np.mean(RNN_loss[:, 4]), '; X velocity --> ', np.mean(np.sqrt(RNN_loss[:, 5])), \\\n",
    "                    '; Y velocity --> ', np.mean(np.sqrt(RNN_loss[:, 6])))\n",
    "\n",
    "print ('RNN integrated model: Lat --> ', np.mean(RNN_loss[:, 2]), '; Lng --> ', np.mean(RNN_loss[:, 3]))\n",
    "\n",
    "print ('====================  Mean absolute error =========================')\n",
    "print ('RNN LSTM       model: Lat --> ', np.mean(RNN_loss[:, 7]), '; Lng --> ', np.mean(RNN_loss[:, 8]), \\\n",
    "                    '; Alt --> ', np.mean(RNN_loss[:, 11]), '; X velocity --> ', np.mean(RNN_loss[:, 12]), \\\n",
    "                    '; Y velocity --> ', np.mean(RNN_loss[:, 13]))\n",
    "\n",
    "print ('RNN integrated model: Lat --> ', np.mean(RNN_loss[:, 9]), '; Lng --> ', np.mean(RNN_loss[:, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8379327721195615"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00920765831440378 * 70)**2 + (0.00928578034162216*70)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
